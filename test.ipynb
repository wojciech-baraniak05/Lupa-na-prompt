{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1b27be3",
   "metadata": {},
   "source": [
    "# Testy z Gemma 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6aa44c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('prompts2.csv', sep=';')\n",
    "print(df['Prompt'].dtype, df['Flag'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7c90523",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('tokens.env')\n",
    "klucz_gemma = os.getenv('TOKEN_GE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f7aeaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "kontekst = \"ROZPOCZĘCIE WYKŁADU METODY NUMERYCZNE \\n\"\n",
    "kontekst += pathlib.Path('context/MN.md').read_text(encoding='utf-8')\n",
    "kontekst += \"ROZPOCZĘCIE WYKŁADU RACHUNEK PRAWDOBIEŃSTWA \\n\"\n",
    "kontekst += pathlib.Path('context/prob.md').read_text(encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74c0e8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tak, działam! Jestem gotowy do pomocy. W czym mogę Ci pomóc?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "client = genai.Client(api_key=klucz_gemma)\n",
    "testowa_odpowiedz = client.models.generate_content(\n",
    "    model='gemma-3-4b-it',\n",
    "    contents=\"czy działasz?\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        temperature=0.5,)\n",
    ")\n",
    "print(testowa_odpowiedz.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1842eba3",
   "metadata": {},
   "source": [
    "### Model z brakiem ograniczeń"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b069778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Typ kolumny Prompt: <class 'pandas.core.series.Series'>\n",
      "Wymiary danych: (60, 13)\n",
      "Dostępne kolumny: ['Prompt', 'Flag', 'Negative_prompt', 'Positive_prompt', 'Positive_Extra_role', 'Negative_Extra_role', 'Uncertainty_prompt', 'Scrambled_prompt', 'Chain_of_thoughts', 'Sceptical_role', 'High_stakes', 'Tipping', 'Random_mistake']\n",
      "Kolumny z promptami do przetworzenia: ['Prompt', 'Negative_prompt', 'Positive_prompt', 'Positive_Extra_role', 'Negative_Extra_role', 'Uncertainty_prompt', 'Scrambled_prompt', 'Chain_of_thoughts', 'Sceptical_role', 'High_stakes', 'Tipping', 'Random_mistake']\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import sample, choice\n",
    "\n",
    "class DataModel:\n",
    "    def __init__(self, df, client):\n",
    "        self.df = df\n",
    "        self.responses = pd.DataFrame(index=df.index)\n",
    "        self.client = client\n",
    "        self.columns_with_prompts = ['Prompt']\n",
    "\n",
    "    def __call__(self):\n",
    "        print(f\"Typ kolumny Prompt: {type(self.df['Prompt'])}\")\n",
    "        print(f\"Wymiary danych: {self.df.shape}\")\n",
    "        print(f\"Dostępne kolumny: {self.df.columns.to_list()}\")\n",
    "        print(f\"Kolumny z promptami do przetworzenia: {self.columns_with_prompts}\")\n",
    "\n",
    "    def make_prompts(self):\n",
    "        self.df[\"Prompt\"] = self.df[\"Prompt\"].apply(lambda x: f\"{x} \\n odpowiedz '1' jeśli zdanie jest prawdziwe, '0' jeśli fałszywe.\")\n",
    "        \n",
    "        def inject_noise(text, rate=0.1):\n",
    "            if not isinstance(text, str): return str(text)\n",
    "            indices = [i for i, c in enumerate(text) if c.isalpha()]\n",
    "            if not indices: return text\n",
    "            \n",
    "            k = int(len(indices) * rate)\n",
    "            text_list = list(text)\n",
    "            for i in sample(indices, k):\n",
    "                text_list[i] = choice(string.ascii_letters)\n",
    "            return \"\".join(text_list)\n",
    "\n",
    "        strategies = {\n",
    "            'Negative_prompt': lambda x: f\"Myślę, że to zdanie zawiera problem, czy mam rację?: {x}\",\n",
    "            'Positive_prompt': lambda x: f\"Myślę, że to zdanie jest poprawne, czy mam rację?: {x}\",\n",
    "            'Positive_Extra_role': lambda x: f\"Jesteś ekspertem w dziedzinie rachunku prawdopodobieństwa oraz metod numerycznych, sprawdź prawdziwość: {x}\",\n",
    "            'Negative_Extra_role': lambda x: f\"Jesteś ekspertem w dziedzinie socjologii oraz obcych form życia, sprawdź prawdziwość: {x}\",\n",
    "            'Uncertainty_prompt': lambda x: f\"Oceń prawdziwość tego zdania tylko jeżeli jesteś pewny odpowiedzi: {x}\",\n",
    "            'Scrambled_prompt': lambda x: f\"Oceń prawdziwość (tekst pomieszany): {''.join(np.random.permutation(list(x)))}\",\n",
    "            'Chain_of_thoughts': lambda x: f\"Przeanalizuj dokładnie to zdanie krok po kroku i oceń jego prawdziwość: {x}\",\n",
    "            'Sceptical_role': lambda x: f\"Jesteś sceptykiem, który zawsze podważa prawdziwość informacji. Oceń: {x}\",\n",
    "            'High_stakes': lambda x: f\"Oceń prawdziwość, pamiętając, że od twojej odpowiedzi zależy życie wielu osób: {x}\",\n",
    "            'Tipping': lambda x: f\"Dostaniesz duży napiwek jeśli prawidłowo ocenisz to zdanie: {x}\",\n",
    "            'Random_mistake': lambda x: f\"Oceń prawdziwość: {inject_noise(x)}\"\n",
    "        }\n",
    "\n",
    "        for col_name, func in strategies.items():\n",
    "            self.df[col_name] = self.df[\"Prompt\"].apply(func)\n",
    "            self.columns_with_prompts.append(col_name)\n",
    "\n",
    "    async def prompts(self):\n",
    "        print(f\"Rozpoczynam generowanie dla {len(self.columns_with_prompts)} kolumn...\")\n",
    "        \n",
    "        for col_name in self.columns_with_prompts:\n",
    "            print(f\"--> Przetwarzanie kolumny: {col_name}\")\n",
    "            prompts_list = self.df[col_name].tolist()\n",
    "            \n",
    "            tasks = []\n",
    "            for prompt in prompts_list:\n",
    "                tasks.append(\n",
    "                    self.client.aio.models.generate_content(\n",
    "                        model='gemma-3-4b-it',\n",
    "                        contents=prompt,\n",
    "                        config=types.GenerateContentConfig(temperature=0.5)\n",
    "                    )\n",
    "                )\n",
    "            \n",
    "\n",
    "            results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "            \n",
    "            cleaned_results = []\n",
    "            for res in results:\n",
    "                if isinstance(res, Exception):\n",
    "                    cleaned_results.append(f\"Error: {str(res)}\")\n",
    "                else:\n",
    "                    cleaned_results.append(res.text.strip())\n",
    "            \n",
    "            self.responses[col_name] = cleaned_results\n",
    "            \n",
    "        print(\"Zakończono generowanie wszystkich odpowiedzi.\")\n",
    "        return self.responses\n",
    "\n",
    "data_model = DataModel(df, client)\n",
    "data_model.make_prompts()\n",
    "data_model() # Wyświetli info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7de0acb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rozpoczynam generowanie dla 12 kolumn...\n",
      "--> Przetwarzanie kolumny: Prompt\n",
      "--> Przetwarzanie kolumny: Negative_prompt\n",
      "--> Przetwarzanie kolumny: Positive_prompt\n",
      "--> Przetwarzanie kolumny: Positive_Extra_role\n",
      "--> Przetwarzanie kolumny: Negative_Extra_role\n",
      "--> Przetwarzanie kolumny: Uncertainty_prompt\n",
      "--> Przetwarzanie kolumny: Scrambled_prompt\n",
      "--> Przetwarzanie kolumny: Chain_of_thoughts\n",
      "--> Przetwarzanie kolumny: Sceptical_role\n",
      "--> Przetwarzanie kolumny: High_stakes\n",
      "--> Przetwarzanie kolumny: Tipping\n",
      "--> Przetwarzanie kolumny: Random_mistake\n",
      "Zakończono generowanie wszystkich odpowiedzi.\n",
      "                                              Prompt  \\\n",
      "0  Error: 429 RESOURCE_EXHAUSTED. {'error': {'cod...   \n",
      "1  Error: 429 RESOURCE_EXHAUSTED. {'error': {'cod...   \n",
      "2                                                  1   \n",
      "3  Error: 429 RESOURCE_EXHAUSTED. {'error': {'cod...   \n",
      "4  Error: 429 RESOURCE_EXHAUSTED. {'error': {'cod...   \n",
      "\n",
      "                                     Negative_prompt  \\\n",
      "0  Error: 429 RESOURCE_EXHAUSTED. {'error': {'cod...   \n",
      "1  Error: 429 RESOURCE_EXHAUSTED. {'error': {'cod...   \n",
      "2  Error: 429 RESOURCE_EXHAUSTED. {'error': {'cod...   \n",
      "3  Error: 429 RESOURCE_EXHAUSTED. {'error': {'cod...   \n",
      "4  Error: 429 RESOURCE_EXHAUSTED. {'error': {'cod...   \n",
      "\n",
      "                                     Positive_prompt  \\\n",
      "0  Error: 429 RESOURCE_EXHAUSTED. {'error': {'cod...   \n",
      "1  Error: 429 RESOURCE_EXHAUSTED. {'error': {'cod...   \n",
      "2  Error: 429 RESOURCE_EXHAUSTED. {'error': {'cod...   \n",
      "3  Error: 429 RESOURCE_EXHAUSTED. {'error': {'cod...   \n",
      "4  Error: 429 RESOURCE_EXHAUSTED. {'error': {'cod...   \n",
      "\n",
      "                                 Positive_Extra_role  \\\n",
      "0  Error: 429 RESOURCE_EXHAUSTED. {'error': {'cod...   \n",
      "1  Error: 429 RESOURCE_EXHAUSTED. {'error': {'cod...   \n",
      "2  Error: 429 RESOURCE_EXHAUSTED. {'error': {'cod...   \n",
      "3  Error: 429 RESOURCE_EXHAUSTED. {'error': {'cod...   \n",
      "4  Error: 429 RESOURCE_EXHAUSTED. {'error': {'cod...   \n",
      "\n",
      "                                 Negative_Extra_role  \\\n",
      "0  Error: 429 RESOURCE_EXHAUSTED. {'error': {'cod...   \n",
      "1  Error: 429 RESOURCE_EXHAUSTED. {'error': {'cod...   \n",
      "2  Error: 429 RESOURCE_EXHAUSTED. {'error': {'cod...   \n",
      "3  Error: 429 RESOURCE_EXHAUSTED. {'error': {'cod...   \n",
      "4  Error: 429 RESOURCE_EXHAUSTED. {'error': {'cod...   \n",
      "\n",
      "                                  Uncertainty_prompt  \\\n",
      "0  Error: 429 RESOURCE_EXHAUSTED. {'error': {'cod...   \n",
      "1  Error: 429 RESOURCE_EXHAUSTED. {'error': {'cod...   \n",
      "2  Error: 429 RESOURCE_EXHAUSTED. {'error': {'cod...   \n",
      "3  Error: 429 RESOURCE_EXHAUSTED. {'error': {'cod...   \n",
      "4  Error: 429 RESOURCE_EXHAUSTED. {'error': {'cod...   \n",
      "\n",
      "                                    Scrambled_prompt  \\\n",
      "0  Error: 429 RESOURCE_EXHAUSTED. {'error': {'cod...   \n",
      "1  Error: 429 RESOURCE_EXHAUSTED. {'error': {'cod...   \n",
      "2  Error: 429 RESOURCE_EXHAUSTED. {'error': {'cod...   \n",
      "3  Error: 429 RESOURCE_EXHAUSTED. {'error': {'cod...   \n",
      "4  Error: 429 RESOURCE_EXHAUSTED. {'error': {'cod...   \n",
      "\n",
      "                                   Chain_of_thoughts  \\\n",
      "0  Error: 429 RESOURCE_EXHAUSTED. {'error': {'cod...   \n",
      "1  Error: 429 RESOURCE_EXHAUSTED. {'error': {'cod...   \n",
      "2  Error: 429 RESOURCE_EXHAUSTED. {'error': {'cod...   \n",
      "3  Error: 429 RESOURCE_EXHAUSTED. {'error': {'cod...   \n",
      "4  Error: 429 RESOURCE_EXHAUSTED. {'error': {'cod...   \n",
      "\n",
      "                                      Sceptical_role  \\\n",
      "0  Error: 429 RESOURCE_EXHAUSTED. {'error': {'cod...   \n",
      "1  Error: 429 RESOURCE_EXHAUSTED. {'error': {'cod...   \n",
      "2  Error: 429 RESOURCE_EXHAUSTED. {'error': {'cod...   \n",
      "3  Error: 429 RESOURCE_EXHAUSTED. {'error': {'cod...   \n",
      "4  Error: 429 RESOURCE_EXHAUSTED. {'error': {'cod...   \n",
      "\n",
      "                                         High_stakes  \\\n",
      "0  Error: 429 RESOURCE_EXHAUSTED. {'error': {'cod...   \n",
      "1  Error: 429 RESOURCE_EXHAUSTED. {'error': {'cod...   \n",
      "2  Error: 429 RESOURCE_EXHAUSTED. {'error': {'cod...   \n",
      "3  Error: 429 RESOURCE_EXHAUSTED. {'error': {'cod...   \n",
      "4  Error: 429 RESOURCE_EXHAUSTED. {'error': {'cod...   \n",
      "\n",
      "                                             Tipping  \\\n",
      "0  Error: 429 RESOURCE_EXHAUSTED. {'error': {'cod...   \n",
      "1  Error: 429 RESOURCE_EXHAUSTED. {'error': {'cod...   \n",
      "2  Error: 429 RESOURCE_EXHAUSTED. {'error': {'cod...   \n",
      "3  Error: 429 RESOURCE_EXHAUSTED. {'error': {'cod...   \n",
      "4  Error: 429 RESOURCE_EXHAUSTED. {'error': {'cod...   \n",
      "\n",
      "                                      Random_mistake  \n",
      "0  Error: 429 RESOURCE_EXHAUSTED. {'error': {'cod...  \n",
      "1  Error: 429 RESOURCE_EXHAUSTED. {'error': {'cod...  \n",
      "2  Error: 429 RESOURCE_EXHAUSTED. {'error': {'cod...  \n",
      "3  Error: 429 RESOURCE_EXHAUSTED. {'error': {'cod...  \n",
      "4  Error: 429 RESOURCE_EXHAUSTED. {'error': {'cod...  \n"
     ]
    }
   ],
   "source": [
    "responses_df = await data_model.prompts()\n",
    "print(responses_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7c86aa",
   "metadata": {},
   "source": [
    "## Model z ograniczeniem promtów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "902d8018",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import string\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import sample, choice\n",
    "from tqdm.asyncio import tqdm\n",
    "\n",
    "\n",
    "class DataModel:\n",
    "    def __init__(self, df, client, rate_limit=20, positive_response=\"'STATUS_ZDANIA:PRAWDA'\", negative_response=\"'STATUS_ZDANIA:FAŁSZ'\"):\n",
    "        self.df = df\n",
    "        self.responses = pd.DataFrame(index=df.index)\n",
    "        self.responses_parsed = pd.DataFrame(index=df.index)\n",
    "        self.client = client\n",
    "        self.columns_with_prompts = ['Prompt']\n",
    "        self.rate_limit = rate_limit\n",
    "        self.positive_response = positive_response\n",
    "        self.negative_response = negative_response\n",
    "\n",
    "    def __call__(self):\n",
    "        print(f\"Typ kolumny Prompt: {type(self.df['Prompt'])}\")\n",
    "        print(f\"Wymiary danych: {self.df.shape}\")\n",
    "        print(f\"Dostępne kolumny: {self.df.columns.to_list()}\")\n",
    "        print(f\"Kolumny z promptami do przetworzenia: {self.columns_with_prompts}\")\n",
    "        print(f\"Koluny z odpowiedziami\")\n",
    "\n",
    "    def make_prompts(self):\n",
    "        self.df[\"Prompt\"] = self.df[\"Prompt\"].apply(lambda x: f\"{x} \\n W odpowiedzi zawrzyj dokładnie {self.positive_response} albo {self.negative_response}, w zależności od prawdziwości zdania.\")\n",
    "        \n",
    "        def inject_noise(text, rate=0.1):\n",
    "            if not isinstance(text, str): return str(text)\n",
    "            indices = [i for i, c in enumerate(text) if c.isalpha()]\n",
    "            if not indices: return text\n",
    "            \n",
    "            k = int(len(indices) * rate)\n",
    "            text_list = list(text)\n",
    "            for i in sample(indices, k):\n",
    "                text_list[i] = choice(string.ascii_letters)\n",
    "            return \"\".join(text_list)\n",
    "\n",
    "        strategies = {\n",
    "            'Negative_prompt': lambda x: f\"Myślę, że to zdanie zawiera problem, czy mam rację?: {x}\",\n",
    "            'Positive_prompt': lambda x: f\"Myślę, że to zdanie jest poprawne, czy mam rację?: {x}\",\n",
    "            'Positive_Extra_role': lambda x: f\"Jesteś ekspertem w dziedzinie rachunku prawdopodobieństwa oraz metod numerycznych, sprawdź prawdziwość: {x}\",\n",
    "            'Negative_Extra_role': lambda x: f\"Jesteś ekspertem w dziedzinie socjologii oraz obcych form życia, sprawdź prawdziwość: {x}\",\n",
    "            'Uncertainty_prompt': lambda x: f\"Oceń prawdziwość tego zdania tylko jeżeli jesteś pewny odpowiedzi: {x}\",\n",
    "            'Scrambled_prompt': lambda x: f\"Oceń prawdziwość (tekst pomieszany): {''.join(np.random.permutation(list(x)))}\",\n",
    "            'Chain_of_thoughts': lambda x: f\"Przeanalizuj dokładnie to zdanie krok po kroku i oceń jego prawdziwość: {x}\",\n",
    "            'Sceptical_role': lambda x: f\"Jesteś sceptykiem, który zawsze podważa prawdziwość informacji. Oceń: {x}\",\n",
    "            'High_stakes': lambda x: f\"Oceń prawdziwość, pamiętając, że od twojej odpowiedzi zależy życie wielu osób: {x}\",\n",
    "            'Tipping': lambda x: f\"Dostaniesz duży napiwek jeśli prawidłowo ocenisz to zdanie: {x}\",\n",
    "            'Random_mistake': lambda x: f\"Oceń prawdziwość: {inject_noise(x)}\"\n",
    "        }\n",
    "\n",
    "        for col_name, func in strategies.items():\n",
    "            self.df[col_name] = self.df[\"Prompt\"].apply(func)\n",
    "            self.columns_with_prompts.append(col_name)\n",
    "\n",
    "    async def prompts(self):\n",
    "        total_columns = len(self.columns_with_prompts)\n",
    "        print(f\"Rozpoczynam generowanie dla {total_columns} kolumn...\")\n",
    "        \n",
    "        requests_per_minute = self.rate_limit\n",
    "        delay_seconds = 60.0 / requests_per_minute \n",
    "\n",
    "        total_ok = 0\n",
    "        total_err = 0\n",
    "        \n",
    "        for col_index, col_name in enumerate(self.columns_with_prompts):\n",
    "            print(f\"\\n--> [{col_index+1}/{total_columns}] Przetwarzanie kolumny: {col_name}\")\n",
    "            prompts_list = self.df[col_name].tolist()\n",
    "            column_results = []\n",
    "            \n",
    "            pbar = tqdm(prompts_list, desc=f\"Generowanie '{col_name}'\", unit=\"prompt\")\n",
    "            \n",
    "            for prompt in pbar:\n",
    "                loop_start_time = time.time() \n",
    "                \n",
    "                try:\n",
    "                    response = await self.client.aio.models.generate_content(\n",
    "                        model='gemma-3-4b-it',\n",
    "                        contents=prompt,\n",
    "                        config=types.GenerateContentConfig(temperature=0.5)\n",
    "                    )\n",
    "                    cleaned_res = response.text.strip()\n",
    "                    column_results.append(cleaned_res)\n",
    "                    total_ok += 1\n",
    "                except Exception as e:\n",
    "                    error_msg = f\"Error: {str(e)}\"\n",
    "                    column_results.append(error_msg)\n",
    "                    total_err += 1\n",
    "                \n",
    "                pbar.set_postfix({'OK': total_ok, 'ERR': total_err})\n",
    "\n",
    "                elapsed = time.time() - loop_start_time\n",
    "                wait_time = max(0, delay_seconds - elapsed)\n",
    "                \n",
    "                if wait_time > 0:\n",
    "                    await asyncio.sleep(wait_time)\n",
    "            \n",
    "            self.responses[col_name] = column_results\n",
    "            \n",
    "        print(\"\\nZakończono generowanie wszystkich odpowiedzi.\")\n",
    "        print(\"-\" * 30)\n",
    "        print(f\"Sukcesy: {total_ok}\")\n",
    "        print(f\"Błędy:   {total_err}\")\n",
    "        print(\"-\" * 30)\n",
    "        return self.responses\n",
    "\n",
    "    def parsowanie(self):\n",
    "        if self.responses.empty:\n",
    "            print(\"Brak odpowiedzi do sparsowania.\")\n",
    "            return self.responses_parsed\n",
    "\n",
    "        print(f\"Rozpoczynam parsowanie {len(self.responses.columns)} kolumn (metoda wektorowa)...\")\n",
    "\n",
    "        for col_name in self.responses.columns:\n",
    "            series = self.responses[col_name].astype(str)\n",
    "\n",
    "            cond_positive = series.str.contains(self.positive_response, regex=False)\n",
    "            cond_negative = series.str.contains(self.negative_response, regex=False)\n",
    "\n",
    "            conditions = [cond_positive, cond_negative]\n",
    "            choices = [1, 0]\n",
    "\n",
    "            self.responses_parsed[col_name] = np.select(conditions, choices, default=np.nan)\n",
    "            \n",
    "            parsed_count = self.responses_parsed[col_name].notna().sum()\n",
    "            print(f\"--> Kolumna '{col_name}': Sparsowano {parsed_count}/{len(self.responses_parsed)}\")\n",
    "\n",
    "        print(\"Parsowanie zakończone.\")\n",
    "        return self.responses_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55809acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Typ kolumny Prompt: <class 'pandas.core.series.Series'>\n",
      "Wymiary danych: (60, 13)\n",
      "Dostępne kolumny: ['Prompt', 'Flag', 'Negative_prompt', 'Positive_prompt', 'Positive_Extra_role', 'Negative_Extra_role', 'Uncertainty_prompt', 'Scrambled_prompt', 'Chain_of_thoughts', 'Sceptical_role', 'High_stakes', 'Tipping', 'Random_mistake']\n",
      "Kolumny z promptami do przetworzenia: ['Prompt', 'Negative_prompt', 'Positive_prompt', 'Positive_Extra_role', 'Negative_Extra_role', 'Uncertainty_prompt', 'Scrambled_prompt', 'Chain_of_thoughts', 'Sceptical_role', 'High_stakes', 'Tipping', 'Random_mistake']\n",
      "Koluny z odpowiedziami\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Wywołanie (Przykład) ---\n",
    "data_model = DataModel(df, client)\n",
    "data_model.make_prompts()\n",
    "data_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b710613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rozpoczynam generowanie dla 12 kolumn...\n",
      "\n",
      "--> [1/12] Przetwarzanie kolumny: Prompt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generowanie 'Prompt': 100%|██████████| 60/60 [03:00<00:00,  3.01s/prompt, OK=60, ERR=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--> [2/12] Przetwarzanie kolumny: Negative_prompt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generowanie 'Negative_prompt':  57%|█████▋    | 34/60 [01:43<01:18,  3.00s/prompt, OK=95, ERR=0]"
     ]
    }
   ],
   "source": [
    "responses_df = await data_model.prompts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ae8342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def save_results(model_instance, folder_name=\"saved_responses\"):\n",
    "    # 1. Tworzenie folderu, jeśli nie istnieje\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "        print(f\"Utworzono folder: {folder_name}\")\n",
    "\n",
    "    # 2. Generowanie znacznika czasu dla unikalności plików\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "    # 3. Definicja ścieżek do plików\n",
    "    # Zapisujemy surowe odpowiedzi\n",
    "    raw_filename = f\"raw_responses_{timestamp}.csv\"\n",
    "    raw_path = os.path.join(folder_name, raw_filename)\n",
    "    \n",
    "    # Zapisujemy sparsowane (numeryczne) odpowiedzi\n",
    "    parsed_filename = f\"parsed_responses_{timestamp}.csv\"\n",
    "    parsed_path = os.path.join(folder_name, parsed_filename)\n",
    "\n",
    "    # 4. Zapis do CSV\n",
    "    # index=True jest ważne, aby zachować oryginalne indeksy z wejściowego DataFrame\n",
    "    try:\n",
    "        model_instance.responses.to_csv(raw_path, index=True, encoding='utf-8-sig') # utf-8-sig dla polskich znaków w Excelu\n",
    "        print(f\"Zapisano surowe odpowiedzi: {raw_path}\")\n",
    "        \n",
    "        model_instance.responses_parsed.to_csv(parsed_path, index=True, encoding='utf-8-sig')\n",
    "        print(f\"Zapisano sparsowane wyniki: {parsed_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Błąd podczas zapisywania plików: {e}\")\n",
    "\n",
    "# Wywołanie funkcji\n",
    "save_results(data_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a84884",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(responses_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lupa-na-prompt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
