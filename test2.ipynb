{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88dcafda",
   "metadata": {},
   "source": [
    "## Testy z Open Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c3e7ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('prompts2.csv', sep=';')\n",
    "print(df['Prompt'].dtype, df['Flag'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "005bfe73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "kontekst = \"ROZPOCZĘCIE WYKŁADU METODY NUMERYCZNE \\n\"\n",
    "kontekst += pathlib.Path('context/MN.md').read_text(encoding='utf-8')\n",
    "kontekst += \"ROZPOCZĘCIE WYKŁADU RACHUNEK PRAWDOBIEŃSTWA \\n\"\n",
    "kontekst += pathlib.Path('context/prob.md').read_text(encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddd3fc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import sample, choice\n",
    "from openai import AsyncOpenAI\n",
    "from tqdm.asyncio import tqdm # Importujemy pasek postępu\n",
    "\n",
    "class DataModel:\n",
    "    def __init__(self, df, client):\n",
    "        self.df = df\n",
    "        self.responses = pd.DataFrame(index=df.index)\n",
    "        self.client = client\n",
    "        self.columns_with_prompts = ['Prompt']\n",
    "\n",
    "    def __call__(self):\n",
    "        print(f\"Typ kolumny Prompt: {type(self.df['Prompt'])}\")\n",
    "        print(f\"Wymiary danych: {self.df.shape}\")\n",
    "\n",
    "    def make_prompts(self):\n",
    "        # Twoja logika promptów (bez zmian)\n",
    "        self.df[\"Prompt\"] = self.df[\"Prompt\"].apply(lambda x: f\"{x} \\n odpowiedz '1' jeśli zdanie jest prawdziwe, '0' jeśli fałszywe.\")\n",
    "        \n",
    "        def inject_noise(text, rate=0.1):\n",
    "            if not isinstance(text, str): return str(text)\n",
    "            indices = [i for i, c in enumerate(text) if c.isalpha()]\n",
    "            if not indices: return text\n",
    "            k = int(len(indices) * rate)\n",
    "            text_list = list(text)\n",
    "            for i in sample(indices, k):\n",
    "                text_list[i] = choice(string.ascii_letters)\n",
    "            return \"\".join(text_list)\n",
    "\n",
    "        strategies = {\n",
    "            'Negative_prompt': lambda x: f\"Myślę, że to zdanie zawiera problem, czy mam rację?: {x}\",\n",
    "            'Positive_prompt': lambda x: f\"Myślę, że to zdanie jest poprawne, czy mam rację?: {x}\",\n",
    "            'Positive_Extra_role': lambda x: f\"Jesteś ekspertem w dziedzinie rachunku prawdopodobieństwa oraz metod numerycznych, sprawdź prawdziwość: {x}\",\n",
    "            'Negative_Extra_role': lambda x: f\"Jesteś ekspertem w dziedzinie socjologii oraz obcych form życia, sprawdź prawdziwość: {x}\",\n",
    "            'Uncertainty_prompt': lambda x: f\"Oceń prawdziwość tego zdania tylko jeżeli jesteś pewny odpowiedzi: {x}\",\n",
    "            'Scrambled_prompt': lambda x: f\"Oceń prawdziwość (tekst pomieszany): {''.join(np.random.permutation(list(x)))}\",\n",
    "            'Chain_of_thoughts': lambda x: f\"Przeanalizuj dokładnie to zdanie krok po kroku i oceń jego prawdziwość: {x}\",\n",
    "            'Sceptical_role': lambda x: f\"Jesteś sceptykiem, który zawsze podważa prawdziwość informacji. Oceń: {x}\",\n",
    "            'High_stakes': lambda x: f\"Oceń prawdziwość, pamiętając, że od twojej odpowiedzi zależy życie wielu osób: {x}\",\n",
    "            'Tipping': lambda x: f\"Dostaniesz duży napiwek jeśli prawidłowo ocenisz to zdanie: {x}\",\n",
    "            'Random_mistake': lambda x: f\"Oceń prawdziwość: {inject_noise(x)}\"\n",
    "        }\n",
    "\n",
    "        for col_name, func in strategies.items():\n",
    "            self.df[col_name] = self.df[\"Prompt\"].apply(func)\n",
    "            self.columns_with_prompts.append(col_name)\n",
    "\n",
    "    async def prompts(self):\n",
    "        # 1. OGRANICZENIE DO SAMEJ KOLUMNY 'PROMPT'\n",
    "        target_columns = ['Prompt'] \n",
    "        print(f\"Rozpoczynam generowanie TYLKO dla kolumn: {target_columns}...\")\n",
    "        \n",
    "        sem = asyncio.Semaphore(2) # Limit równoległych zapytań\n",
    "\n",
    "        # Liczniki do paska postępu (w formie słownika, żeby były mutowalne wewnątrz funkcji)\n",
    "        stats = {'OK': 0, 'ERR': 0}\n",
    "\n",
    "        async def fetch_one(prompt, pbar):\n",
    "            async with sem:\n",
    "                result_text = \"\"\n",
    "                success = False\n",
    "                \n",
    "                # Próby (Retries)\n",
    "                for attempt in range(3):\n",
    "                    try:\n",
    "                        response = await self.client.chat.completions.create(\n",
    "                            model=\"z-ai/glm-4.5-air:free\",\n",
    "                            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                            extra_body={}\n",
    "                        )\n",
    "                        result_text = response.choices[0].message.content.strip()\n",
    "                        success = True\n",
    "                        break # Udało się, wychodzimy z pętli prób\n",
    "                    except Exception as e:\n",
    "                        if attempt == 2: # Ostatnia próba\n",
    "                            result_text = f\"Error: {str(e)}\"\n",
    "                        else:\n",
    "                            await asyncio.sleep(2 * (attempt + 1))\n",
    "\n",
    "                # Aktualizacja paska postępu i liczników\n",
    "                if success:\n",
    "                    stats['OK'] += 1\n",
    "                else:\n",
    "                    stats['ERR'] += 1\n",
    "                \n",
    "                pbar.set_postfix(OK=stats['OK'], ERR=stats['ERR'])\n",
    "                pbar.update(1)\n",
    "                \n",
    "                return result_text\n",
    "\n",
    "        for col_name in target_columns:\n",
    "            print(f\"--> Przetwarzanie kolumny: {col_name}\")\n",
    "            prompts_list = self.df[col_name].tolist()\n",
    "            \n",
    "            # Inicjalizacja paska postępu dla danej kolumny\n",
    "            with tqdm(total=len(prompts_list), desc=f\"Processing {col_name}\") as pbar:\n",
    "                # Tworzymy zadania z przekazaniem paska postępu\n",
    "                tasks = [fetch_one(prompt, pbar) for prompt in prompts_list]\n",
    "                \n",
    "                # Gather gwarantuje zachowanie kolejności wyników (dla DataFrame)\n",
    "                results = await asyncio.gather(*tasks)\n",
    "            \n",
    "            self.responses[col_name] = results\n",
    "            \n",
    "        print(\"Zakończono.\")\n",
    "        return self.responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0e6253",
   "metadata": {},
   "outputs": [],
   "source": [
    "klucz = 'klucz api open router'\n",
    "client = AsyncOpenAI(\n",
    "  base_url=\"https://openrouter.ai/api/v1\",\n",
    "  api_key=klucz,\n",
    "  timeout=60.0,\n",
    "  default_headers={\"HTTP-Referer\": \"http://localhost:8888\", \"X-Title\": \"Analysis\"}\n",
    ")\n",
    "\n",
    "\n",
    "data_model = DataModel(df, client)\n",
    "data_model.make_prompts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a386a55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rozpoczynam generowanie TYLKO dla kolumn: ['Prompt']...\n",
      "--> Przetwarzanie kolumny: Prompt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Prompt:  10%|█         | 6/60 [01:03<09:28, 10.53s/it, ERR=6, OK=0]\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Lupa-na-prompt/.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:228\u001b[39m, in \u001b[36mAsyncConnectionPool.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._optional_thread_lock:\n\u001b[32m    226\u001b[39m     \u001b[38;5;66;03m# Assign incoming requests to available connections,\u001b[39;00m\n\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# closing or creating new connections as required.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m228\u001b[39m     closing = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_assign_requests_to_connections\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Lupa-na-prompt/.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:288\u001b[39m, in \u001b[36mAsyncConnectionPool._assign_requests_to_connections\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    287\u001b[39m     \u001b[38;5;28mself\u001b[39m._connections.remove(connection)\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhas_expired\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    289\u001b[39m     \u001b[38;5;66;03m# log: \"closing expired connection\"\u001b[39;00m\n\u001b[32m    290\u001b[39m     \u001b[38;5;28mself\u001b[39m._connections.remove(connection)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Lupa-na-prompt/.venv/lib/python3.12/site-packages/httpcore/_async/connection.py:190\u001b[39m, in \u001b[36mAsyncHTTPConnection.has_expired\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    189\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connect_failed\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhas_expired\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Lupa-na-prompt/.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:283\u001b[39m, in \u001b[36mAsyncHTTP11Connection.has_expired\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    278\u001b[39m \u001b[38;5;66;03m# If the HTTP connection is idle but the socket is readable, then the\u001b[39;00m\n\u001b[32m    279\u001b[39m \u001b[38;5;66;03m# only valid state is that the socket is about to return b\"\", indicating\u001b[39;00m\n\u001b[32m    280\u001b[39m \u001b[38;5;66;03m# a server-initiated disconnect.\u001b[39;00m\n\u001b[32m    281\u001b[39m server_disconnected = (\n\u001b[32m    282\u001b[39m     \u001b[38;5;28mself\u001b[39m._state == HTTPConnectionState.IDLE\n\u001b[32m--> \u001b[39m\u001b[32m283\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_extra_info\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mis_readable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    284\u001b[39m )\n\u001b[32m    286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m keepalive_expired \u001b[38;5;129;01mor\u001b[39;00m server_disconnected\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Lupa-na-prompt/.venv/lib/python3.12/site-packages/httpcore/_backends/anyio.py:92\u001b[39m, in \u001b[36mAnyIOStream.get_extra_info\u001b[39m\u001b[34m(self, info)\u001b[39m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m info == \u001b[33m\"\u001b[39m\u001b[33mis_readable\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     sock = \u001b[38;5;28mself\u001b[39m._stream.extra(\u001b[43manyio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mabc\u001b[49m.SocketAttribute.raw_socket, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m is_socket_readable(sock)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Lupa-na-prompt/.venv/lib/python3.12/site-packages/anyio/__init__.py:111\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(attr)\u001b[39m\n\u001b[32m    109\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m BrokenWorkerInterpreter\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'anyio' has no attribute 'abc'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Lupa-na-prompt/.venv/lib/python3.12/site-packages/openai/_base_client.py:1532\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1531\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1532\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.send(\n\u001b[32m   1533\u001b[39m         request,\n\u001b[32m   1534\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._should_stream_response_body(request=request),\n\u001b[32m   1535\u001b[39m         **kwargs,\n\u001b[32m   1536\u001b[39m     )\n\u001b[32m   1537\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Lupa-na-prompt/.venv/lib/python3.12/site-packages/httpx/_client.py:1629\u001b[39m, in \u001b[36mAsyncClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m   1627\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m-> \u001b[39m\u001b[32m1629\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_handling_auth(\n\u001b[32m   1630\u001b[39m     request,\n\u001b[32m   1631\u001b[39m     auth=auth,\n\u001b[32m   1632\u001b[39m     follow_redirects=follow_redirects,\n\u001b[32m   1633\u001b[39m     history=[],\n\u001b[32m   1634\u001b[39m )\n\u001b[32m   1635\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Lupa-na-prompt/.venv/lib/python3.12/site-packages/httpx/_client.py:1657\u001b[39m, in \u001b[36mAsyncClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m   1656\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1657\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_handling_redirects(\n\u001b[32m   1658\u001b[39m         request,\n\u001b[32m   1659\u001b[39m         follow_redirects=follow_redirects,\n\u001b[32m   1660\u001b[39m         history=history,\n\u001b[32m   1661\u001b[39m     )\n\u001b[32m   1662\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Lupa-na-prompt/.venv/lib/python3.12/site-packages/httpx/_client.py:1694\u001b[39m, in \u001b[36mAsyncClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m   1692\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m hook(request)\n\u001b[32m-> \u001b[39m\u001b[32m1694\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_single_request(request)\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Lupa-na-prompt/.venv/lib/python3.12/site-packages/httpx/_client.py:1730\u001b[39m, in \u001b[36mAsyncClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1729\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1730\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m transport.handle_async_request(request)\n\u001b[32m   1732\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, AsyncByteStream)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Lupa-na-prompt/.venv/lib/python3.12/site-packages/httpx/_transports/default.py:394\u001b[39m, in \u001b[36mAsyncHTTPTransport.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    393\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m394\u001b[39m     resp = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pool.handle_async_request(req)\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.AsyncIterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Lupa-na-prompt/.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:253\u001b[39m, in \u001b[36mAsyncConnectionPool.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    252\u001b[39m     \u001b[38;5;28mself\u001b[39m._requests.remove(pool_request)\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m     closing = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_assign_requests_to_connections\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Lupa-na-prompt/.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:288\u001b[39m, in \u001b[36mAsyncConnectionPool._assign_requests_to_connections\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    287\u001b[39m     \u001b[38;5;28mself\u001b[39m._connections.remove(connection)\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhas_expired\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    289\u001b[39m     \u001b[38;5;66;03m# log: \"closing expired connection\"\u001b[39;00m\n\u001b[32m    290\u001b[39m     \u001b[38;5;28mself\u001b[39m._connections.remove(connection)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Lupa-na-prompt/.venv/lib/python3.12/site-packages/httpcore/_async/connection.py:190\u001b[39m, in \u001b[36mAsyncHTTPConnection.has_expired\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    189\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connect_failed\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhas_expired\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Lupa-na-prompt/.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:283\u001b[39m, in \u001b[36mAsyncHTTP11Connection.has_expired\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    278\u001b[39m \u001b[38;5;66;03m# If the HTTP connection is idle but the socket is readable, then the\u001b[39;00m\n\u001b[32m    279\u001b[39m \u001b[38;5;66;03m# only valid state is that the socket is about to return b\"\", indicating\u001b[39;00m\n\u001b[32m    280\u001b[39m \u001b[38;5;66;03m# a server-initiated disconnect.\u001b[39;00m\n\u001b[32m    281\u001b[39m server_disconnected = (\n\u001b[32m    282\u001b[39m     \u001b[38;5;28mself\u001b[39m._state == HTTPConnectionState.IDLE\n\u001b[32m--> \u001b[39m\u001b[32m283\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_extra_info\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mis_readable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    284\u001b[39m )\n\u001b[32m    286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m keepalive_expired \u001b[38;5;129;01mor\u001b[39;00m server_disconnected\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Lupa-na-prompt/.venv/lib/python3.12/site-packages/httpcore/_backends/anyio.py:92\u001b[39m, in \u001b[36mAnyIOStream.get_extra_info\u001b[39m\u001b[34m(self, info)\u001b[39m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m info == \u001b[33m\"\u001b[39m\u001b[33mis_readable\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     sock = \u001b[38;5;28mself\u001b[39m._stream.extra(\u001b[43manyio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mabc\u001b[49m.SocketAttribute.raw_socket, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m is_socket_readable(sock)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Lupa-na-prompt/.venv/lib/python3.12/site-packages/anyio/__init__.py:111\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(attr)\u001b[39m\n\u001b[32m    109\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m BrokenWorkerInterpreter\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'anyio' has no attribute 'abc'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mCancelledError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m responses = \u001b[38;5;28;01mawait\u001b[39;00m data_model.prompts()\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(responses.head())\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 105\u001b[39m, in \u001b[36mDataModel.prompts\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    102\u001b[39m         tasks = [fetch_one(prompt, pbar) \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts_list]\n\u001b[32m    104\u001b[39m         \u001b[38;5;66;03m# Gather gwarantuje zachowanie kolejności wyników (dla DataFrame)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m         results = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(*tasks)\n\u001b[32m    107\u001b[39m     \u001b[38;5;28mself\u001b[39m.responses[col_name] = results\n\u001b[32m    109\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mZakończono.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 70\u001b[39m, in \u001b[36mDataModel.prompts.<locals>.fetch_one\u001b[39m\u001b[34m(prompt, pbar)\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m3\u001b[39m):\n\u001b[32m     69\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m         response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.client.chat.completions.create(\n\u001b[32m     71\u001b[39m             model=\u001b[33m\"\u001b[39m\u001b[33mz-ai/glm-4.5-air:free\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     72\u001b[39m             messages=[{\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: prompt}],\n\u001b[32m     73\u001b[39m             extra_body={}\n\u001b[32m     74\u001b[39m         )\n\u001b[32m     75\u001b[39m         result_text = response.choices[\u001b[32m0\u001b[39m].message.content.strip()\n\u001b[32m     76\u001b[39m         success = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Lupa-na-prompt/.venv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py:2678\u001b[39m, in \u001b[36mAsyncCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   2631\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   2632\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   2633\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2675\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   2676\u001b[39m ) -> ChatCompletion | AsyncStream[ChatCompletionChunk]:\n\u001b[32m   2677\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m2678\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   2679\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2680\u001b[39m         body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   2681\u001b[39m             {\n\u001b[32m   2682\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages,\n\u001b[32m   2683\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   2684\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m\"\u001b[39m: audio,\n\u001b[32m   2685\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   2686\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunction_call\u001b[39m\u001b[33m\"\u001b[39m: function_call,\n\u001b[32m   2687\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunctions\u001b[39m\u001b[33m\"\u001b[39m: functions,\n\u001b[32m   2688\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m\"\u001b[39m: logit_bias,\n\u001b[32m   2689\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogprobs\u001b[39m\u001b[33m\"\u001b[39m: logprobs,\n\u001b[32m   2690\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_completion_tokens,\n\u001b[32m   2691\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens,\n\u001b[32m   2692\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   2693\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodalities\u001b[39m\u001b[33m\"\u001b[39m: modalities,\n\u001b[32m   2694\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n,\n\u001b[32m   2695\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   2696\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m: prediction,\n\u001b[32m   2697\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m\"\u001b[39m: presence_penalty,\n\u001b[32m   2698\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprompt_cache_key\u001b[39m\u001b[33m\"\u001b[39m: prompt_cache_key,\n\u001b[32m   2699\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprompt_cache_retention\u001b[39m\u001b[33m\"\u001b[39m: prompt_cache_retention,\n\u001b[32m   2700\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mreasoning_effort\u001b[39m\u001b[33m\"\u001b[39m: reasoning_effort,\n\u001b[32m   2701\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m: response_format,\n\u001b[32m   2702\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33msafety_identifier\u001b[39m\u001b[33m\"\u001b[39m: safety_identifier,\n\u001b[32m   2703\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m: seed,\n\u001b[32m   2704\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   2705\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop,\n\u001b[32m   2706\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   2707\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   2708\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m   2709\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   2710\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   2711\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   2712\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   2713\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   2714\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   2715\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mverbosity\u001b[39m\u001b[33m\"\u001b[39m: verbosity,\n\u001b[32m   2716\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mweb_search_options\u001b[39m\u001b[33m\"\u001b[39m: web_search_options,\n\u001b[32m   2717\u001b[39m             },\n\u001b[32m   2718\u001b[39m             completion_create_params.CompletionCreateParamsStreaming\n\u001b[32m   2719\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[32m   2720\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m completion_create_params.CompletionCreateParamsNonStreaming,\n\u001b[32m   2721\u001b[39m         ),\n\u001b[32m   2722\u001b[39m         options=make_request_options(\n\u001b[32m   2723\u001b[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   2724\u001b[39m         ),\n\u001b[32m   2725\u001b[39m         cast_to=ChatCompletion,\n\u001b[32m   2726\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2727\u001b[39m         stream_cls=AsyncStream[ChatCompletionChunk],\n\u001b[32m   2728\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Lupa-na-prompt/.venv/lib/python3.12/site-packages/openai/_base_client.py:1797\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1784\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1785\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1792\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1793\u001b[39m ) -> ResponseT | _AsyncStreamT:\n\u001b[32m   1794\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1795\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1796\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1797\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Lupa-na-prompt/.venv/lib/python3.12/site-packages/openai/_base_client.py:1555\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1552\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered Exception\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   1554\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1555\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sleep_for_retry(\n\u001b[32m   1556\u001b[39m         retries_taken=retries_taken,\n\u001b[32m   1557\u001b[39m         max_retries=max_retries,\n\u001b[32m   1558\u001b[39m         options=input_options,\n\u001b[32m   1559\u001b[39m         response=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1560\u001b[39m     )\n\u001b[32m   1561\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1563\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33mRaising connection error\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Lupa-na-prompt/.venv/lib/python3.12/site-packages/openai/_base_client.py:1623\u001b[39m, in \u001b[36mAsyncAPIClient._sleep_for_retry\u001b[39m\u001b[34m(self, retries_taken, max_retries, options, response)\u001b[39m\n\u001b[32m   1620\u001b[39m timeout = \u001b[38;5;28mself\u001b[39m._calculate_retry_timeout(remaining_retries, options, response.headers \u001b[38;5;28;01mif\u001b[39;00m response \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m   1621\u001b[39m log.info(\u001b[33m\"\u001b[39m\u001b[33mRetrying request to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m in \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[33m seconds\u001b[39m\u001b[33m\"\u001b[39m, options.url, timeout)\n\u001b[32m-> \u001b[39m\u001b[32m1623\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m anyio.sleep(timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Lupa-na-prompt/.venv/lib/python3.12/site-packages/anyio/_core/_eventloop.py:87\u001b[39m, in \u001b[36msleep\u001b[39m\u001b[34m(delay)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msleep\u001b[39m(delay: \u001b[38;5;28mfloat\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     81\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     82\u001b[39m \u001b[33;03m    Pause the current task for the specified duration.\u001b[39;00m\n\u001b[32m     83\u001b[39m \n\u001b[32m     84\u001b[39m \u001b[33;03m    :param delay: the duration, in seconds\u001b[39;00m\n\u001b[32m     85\u001b[39m \n\u001b[32m     86\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m get_async_backend().sleep(delay)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Lupa-na-prompt/.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py:2409\u001b[39m, in \u001b[36mAsyncIOBackend.sleep\u001b[39m\u001b[34m(cls, delay)\u001b[39m\n\u001b[32m   2407\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m   2408\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msleep\u001b[39m(\u001b[38;5;28mcls\u001b[39m, delay: \u001b[38;5;28mfloat\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2409\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m sleep(delay)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/lib/python3.12/asyncio/tasks.py:665\u001b[39m, in \u001b[36msleep\u001b[39m\u001b[34m(delay, result)\u001b[39m\n\u001b[32m    661\u001b[39m h = loop.call_later(delay,\n\u001b[32m    662\u001b[39m                     futures._set_result_unless_cancelled,\n\u001b[32m    663\u001b[39m                     future, result)\n\u001b[32m    664\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m665\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m future\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    667\u001b[39m     h.cancel()\n",
      "\u001b[31mCancelledError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "responses = await data_model.prompts()\n",
    "print(responses.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a3f9819",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import sample, choice\n",
    "from openai import AsyncOpenAI  # Importujemy klienta asynchronicznego\n",
    "\n",
    "class DataModel:\n",
    "    def __init__(self, df, client):\n",
    "        self.df = df\n",
    "        self.responses = pd.DataFrame(index=df.index)\n",
    "        self.client = client # Tutaj teraz będzie instancja AsyncOpenAI\n",
    "        self.columns_with_prompts = ['Prompt']\n",
    "\n",
    "    def __call__(self):\n",
    "        print(f\"Typ kolumny Prompt: {type(self.df['Prompt'])}\")\n",
    "        print(f\"Wymiary danych: {self.df.shape}\")\n",
    "        print(f\"Dostępne kolumny: {self.df.columns.to_list()}\")\n",
    "        print(f\"Kolumny z promptami do przetworzenia: {self.columns_with_prompts}\")\n",
    "\n",
    "    def make_prompts(self):\n",
    "        # Ta część pozostaje bez zmian\n",
    "        self.df[\"Prompt\"] = self.df[\"Prompt\"].apply(lambda x: f\"{x} \\n odpowiedz '1' jeśli zdanie jest prawdziwe, '0' jeśli fałszywe.\")\n",
    "        \n",
    "        def inject_noise(text, rate=0.1):\n",
    "            if not isinstance(text, str): return str(text)\n",
    "            indices = [i for i, c in enumerate(text) if c.isalpha()]\n",
    "            if not indices: return text\n",
    "            k = int(len(indices) * rate)\n",
    "            text_list = list(text)\n",
    "            for i in sample(indices, k):\n",
    "                text_list[i] = choice(string.ascii_letters)\n",
    "            return \"\".join(text_list)\n",
    "\n",
    "        strategies = {\n",
    "            'Negative_prompt': lambda x: f\"Myślę, że to zdanie zawiera problem, czy mam rację?: {x}\",\n",
    "            'Positive_prompt': lambda x: f\"Myślę, że to zdanie jest poprawne, czy mam rację?: {x}\",\n",
    "            'Positive_Extra_role': lambda x: f\"Jesteś ekspertem w dziedzinie rachunku prawdopodobieństwa oraz metod numerycznych, sprawdź prawdziwość: {x}\",\n",
    "            'Negative_Extra_role': lambda x: f\"Jesteś ekspertem w dziedzinie socjologii oraz obcych form życia, sprawdź prawdziwość: {x}\",\n",
    "            'Uncertainty_prompt': lambda x: f\"Oceń prawdziwość tego zdania tylko jeżeli jesteś pewny odpowiedzi: {x}\",\n",
    "            'Scrambled_prompt': lambda x: f\"Oceń prawdziwość (tekst pomieszany): {''.join(np.random.permutation(list(x)))}\",\n",
    "            'Chain_of_thoughts': lambda x: f\"Przeanalizuj dokładnie to zdanie krok po kroku i oceń jego prawdziwość: {x}\",\n",
    "            'Sceptical_role': lambda x: f\"Jesteś sceptykiem, który zawsze podważa prawdziwość informacji. Oceń: {x}\",\n",
    "            'High_stakes': lambda x: f\"Oceń prawdziwość, pamiętając, że od twojej odpowiedzi zależy życie wielu osób: {x}\",\n",
    "            'Tipping': lambda x: f\"Dostaniesz duży napiwek jeśli prawidłowo ocenisz to zdanie: {x}\",\n",
    "            'Random_mistake': lambda x: f\"Oceń prawdziwość: {inject_noise(x)}\"\n",
    "        }\n",
    "\n",
    "        for col_name, func in strategies.items():\n",
    "            self.df[col_name] = self.df[\"Prompt\"].apply(func)\n",
    "            self.columns_with_prompts.append(col_name)\n",
    "\n",
    "    async def prompts(self):\n",
    "        print(f\"Rozpoczynam generowanie dla {len(self.columns_with_prompts)} kolumn...\")\n",
    "        \n",
    "        # Opcjonalnie: Semafor, żeby nie zabić darmowego API OpenRoutera zbyt wieloma zapytaniami naraz\n",
    "        sem = asyncio.Semaphore(5) \n",
    "\n",
    "        async def fetch_one(prompt):\n",
    "            async with sem:\n",
    "                try:\n",
    "                    # Nowe wywołanie zgodne z OpenAI / OpenRouter\n",
    "                    response = await self.client.chat.completions.create(\n",
    "                        model=\"z-ai/glm-4.5-air:free\",\n",
    "                        messages=[\n",
    "                            {\"role\": \"user\", \"content\": prompt}\n",
    "                        ],\n",
    "                        extra_body={} # Wymagane przez niektóre endpointy OpenRouter\n",
    "                    )\n",
    "                    # Wyciągamy treść odpowiedzi w nowym formacie\n",
    "                    return response.choices[0].message.content.strip()\n",
    "                except Exception as e:\n",
    "                    return f\"Error: {str(e)}\"\n",
    "\n",
    "        for col_name in self.columns_with_prompts:\n",
    "            print(f\"--> Przetwarzanie kolumny: {col_name}\")\n",
    "            prompts_list = self.df[col_name].tolist()\n",
    "            \n",
    "            # Tworzymy listę zadań (tasks)\n",
    "            tasks = [fetch_one(prompt) for prompt in prompts_list]\n",
    "            \n",
    "            # Uruchamiamy równolegle\n",
    "            results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "            \n",
    "            self.responses[col_name] = results\n",
    "            \n",
    "        print(\"Zakończono generowanie wszystkich odpowiedzi.\")\n",
    "        return self.responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff03d028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Typ kolumny Prompt: <class 'pandas.core.series.Series'>\n",
      "Wymiary danych: (60, 13)\n",
      "Dostępne kolumny: ['Prompt', 'Flag', 'Negative_prompt', 'Positive_prompt', 'Positive_Extra_role', 'Negative_Extra_role', 'Uncertainty_prompt', 'Scrambled_prompt', 'Chain_of_thoughts', 'Sceptical_role', 'High_stakes', 'Tipping', 'Random_mistake']\n",
      "Kolumny z promptami do przetworzenia: ['Prompt', 'Negative_prompt', 'Positive_prompt', 'Positive_Extra_role', 'Negative_Extra_role', 'Uncertainty_prompt', 'Scrambled_prompt', 'Chain_of_thoughts', 'Sceptical_role', 'High_stakes', 'Tipping', 'Random_mistake']\n"
     ]
    }
   ],
   "source": [
    "klucz = 'klucz api open router'\n",
    "\n",
    "client = AsyncOpenAI(\n",
    "  base_url=\"https://openrouter.ai/api/v1\",\n",
    "  api_key=klucz,\n",
    ")\n",
    "\n",
    "data_model = DataModel(df, client)\n",
    "data_model.make_prompts()\n",
    "data_model() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "091d2160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rozpoczynam generowanie dla 12 kolumn...\n",
      "--> Przetwarzanie kolumny: Prompt\n",
      "--> Przetwarzanie kolumny: Negative_prompt\n",
      "--> Przetwarzanie kolumny: Positive_prompt\n",
      "--> Przetwarzanie kolumny: Positive_Extra_role\n",
      "--> Przetwarzanie kolumny: Negative_Extra_role\n",
      "--> Przetwarzanie kolumny: Uncertainty_prompt\n",
      "--> Przetwarzanie kolumny: Scrambled_prompt\n",
      "--> Przetwarzanie kolumny: Chain_of_thoughts\n",
      "--> Przetwarzanie kolumny: Sceptical_role\n",
      "--> Przetwarzanie kolumny: High_stakes\n",
      "--> Przetwarzanie kolumny: Tipping\n",
      "--> Przetwarzanie kolumny: Random_mistake\n",
      "Zakończono generowanie wszystkich odpowiedzi.\n",
      "                     Prompt           Negative_prompt  \\\n",
      "0  Error: Connection error.  Error: Connection error.   \n",
      "1  Error: Connection error.  Error: Connection error.   \n",
      "2  Error: Connection error.  Error: Connection error.   \n",
      "3  Error: Connection error.  Error: Connection error.   \n",
      "4  Error: Connection error.  Error: Connection error.   \n",
      "\n",
      "            Positive_prompt       Positive_Extra_role  \\\n",
      "0  Error: Connection error.  Error: Connection error.   \n",
      "1  Error: Connection error.  Error: Connection error.   \n",
      "2  Error: Connection error.  Error: Connection error.   \n",
      "3  Error: Connection error.  Error: Connection error.   \n",
      "4  Error: Connection error.  Error: Connection error.   \n",
      "\n",
      "        Negative_Extra_role        Uncertainty_prompt  \\\n",
      "0  Error: Connection error.  Error: Connection error.   \n",
      "1  Error: Connection error.  Error: Connection error.   \n",
      "2  Error: Connection error.  Error: Connection error.   \n",
      "3  Error: Connection error.  Error: Connection error.   \n",
      "4  Error: Connection error.  Error: Connection error.   \n",
      "\n",
      "           Scrambled_prompt         Chain_of_thoughts  \\\n",
      "0  Error: Connection error.  Error: Connection error.   \n",
      "1  Error: Connection error.  Error: Connection error.   \n",
      "2  Error: Connection error.  Error: Connection error.   \n",
      "3  Error: Connection error.  Error: Connection error.   \n",
      "4  Error: Connection error.  Error: Connection error.   \n",
      "\n",
      "             Sceptical_role               High_stakes  \\\n",
      "0  Error: Connection error.  Error: Connection error.   \n",
      "1  Error: Connection error.  Error: Connection error.   \n",
      "2  Error: Connection error.  Error: Connection error.   \n",
      "3  Error: Connection error.  Error: Connection error.   \n",
      "4  Error: Connection error.  Error: Connection error.   \n",
      "\n",
      "                    Tipping            Random_mistake  \n",
      "0  Error: Connection error.  Error: Connection error.  \n",
      "1  Error: Connection error.  Error: Connection error.  \n",
      "2  Error: Connection error.  Error: Connection error.  \n",
      "3  Error: Connection error.  Error: Connection error.  \n",
      "4  Error: Connection error.  Error: Connection error.  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "responses = await data_model.prompts()\n",
    "print(responses.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lupa-na-prompt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
