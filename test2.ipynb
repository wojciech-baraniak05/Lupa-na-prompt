{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88dcafda",
   "metadata": {},
   "source": [
    "## Testy z Open Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c3e7ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('prompts2.csv', sep=';')\n",
    "print(df['Prompt'].dtype, df['Flag'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "005bfe73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "kontekst = \"ROZPOCZĘCIE WYKŁADU METODY NUMERYCZNE \\n\"\n",
    "kontekst += pathlib.Path('context/MN.md').read_text(encoding='utf-8')\n",
    "kontekst += \"ROZPOCZĘCIE WYKŁADU RACHUNEK PRAWDOBIEŃSTWA \\n\"\n",
    "kontekst += pathlib.Path('context/prob.md').read_text(encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddd3fc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import sample, choice\n",
    "from openai import AsyncOpenAI\n",
    "from tqdm.asyncio import tqdm # Importujemy pasek postępu\n",
    "\n",
    "class DataModel:\n",
    "    def __init__(self, df, client):\n",
    "        self.df = df\n",
    "        self.responses = pd.DataFrame(index=df.index)\n",
    "        self.client = client\n",
    "        self.columns_with_prompts = ['Prompt']\n",
    "\n",
    "    def __call__(self):\n",
    "        print(f\"Typ kolumny Prompt: {type(self.df['Prompt'])}\")\n",
    "        print(f\"Wymiary danych: {self.df.shape}\")\n",
    "\n",
    "    def make_prompts(self):\n",
    "        # Twoja logika promptów (bez zmian)\n",
    "        self.df[\"Prompt\"] = self.df[\"Prompt\"].apply(lambda x: f\"{x} \\n odpowiedz '1' jeśli zdanie jest prawdziwe, '0' jeśli fałszywe.\")\n",
    "        \n",
    "        def inject_noise(text, rate=0.1):\n",
    "            if not isinstance(text, str): return str(text)\n",
    "            indices = [i for i, c in enumerate(text) if c.isalpha()]\n",
    "            if not indices: return text\n",
    "            k = int(len(indices) * rate)\n",
    "            text_list = list(text)\n",
    "            for i in sample(indices, k):\n",
    "                text_list[i] = choice(string.ascii_letters)\n",
    "            return \"\".join(text_list)\n",
    "\n",
    "        strategies = {\n",
    "            'Negative_prompt': lambda x: f\"Myślę, że to zdanie zawiera problem, czy mam rację?: {x}\",\n",
    "            'Positive_prompt': lambda x: f\"Myślę, że to zdanie jest poprawne, czy mam rację?: {x}\",\n",
    "            'Positive_Extra_role': lambda x: f\"Jesteś ekspertem w dziedzinie rachunku prawdopodobieństwa oraz metod numerycznych, sprawdź prawdziwość: {x}\",\n",
    "            'Negative_Extra_role': lambda x: f\"Jesteś ekspertem w dziedzinie socjologii oraz obcych form życia, sprawdź prawdziwość: {x}\",\n",
    "            'Uncertainty_prompt': lambda x: f\"Oceń prawdziwość tego zdania tylko jeżeli jesteś pewny odpowiedzi: {x}\",\n",
    "            'Scrambled_prompt': lambda x: f\"Oceń prawdziwość (tekst pomieszany): {''.join(np.random.permutation(list(x)))}\",\n",
    "            'Chain_of_thoughts': lambda x: f\"Przeanalizuj dokładnie to zdanie krok po kroku i oceń jego prawdziwość: {x}\",\n",
    "            'Sceptical_role': lambda x: f\"Jesteś sceptykiem, który zawsze podważa prawdziwość informacji. Oceń: {x}\",\n",
    "            'High_stakes': lambda x: f\"Oceń prawdziwość, pamiętając, że od twojej odpowiedzi zależy życie wielu osób: {x}\",\n",
    "            'Tipping': lambda x: f\"Dostaniesz duży napiwek jeśli prawidłowo ocenisz to zdanie: {x}\",\n",
    "            'Random_mistake': lambda x: f\"Oceń prawdziwość: {inject_noise(x)}\"\n",
    "        }\n",
    "\n",
    "        for col_name, func in strategies.items():\n",
    "            self.df[col_name] = self.df[\"Prompt\"].apply(func)\n",
    "            self.columns_with_prompts.append(col_name)\n",
    "\n",
    "    async def prompts(self):\n",
    "        # 1. OGRANICZENIE DO SAMEJ KOLUMNY 'PROMPT'\n",
    "        target_columns = ['Prompt'] \n",
    "        print(f\"Rozpoczynam generowanie TYLKO dla kolumn: {target_columns}...\")\n",
    "        \n",
    "        sem = asyncio.Semaphore(2) # Limit równoległych zapytań\n",
    "\n",
    "        # Liczniki do paska postępu (w formie słownika, żeby były mutowalne wewnątrz funkcji)\n",
    "        stats = {'OK': 0, 'ERR': 0}\n",
    "\n",
    "        async def fetch_one(prompt, pbar):\n",
    "            async with sem:\n",
    "                result_text = \"\"\n",
    "                success = False\n",
    "                \n",
    "                # Próby (Retries)\n",
    "                for attempt in range(3):\n",
    "                    try:\n",
    "                        response = await self.client.chat.completions.create(\n",
    "                            model=\"z-ai/glm-4.5-air:free\",\n",
    "                            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                            extra_body={}\n",
    "                        )\n",
    "                        result_text = response.choices[0].message.content.strip()\n",
    "                        success = True\n",
    "                        break # Udało się, wychodzimy z pętli prób\n",
    "                    except Exception as e:\n",
    "                        if attempt == 2: # Ostatnia próba\n",
    "                            result_text = f\"Error: {str(e)}\"\n",
    "                        else:\n",
    "                            await asyncio.sleep(2 * (attempt + 1))\n",
    "\n",
    "                # Aktualizacja paska postępu i liczników\n",
    "                if success:\n",
    "                    stats['OK'] += 1\n",
    "                else:\n",
    "                    stats['ERR'] += 1\n",
    "                \n",
    "                pbar.set_postfix(OK=stats['OK'], ERR=stats['ERR'])\n",
    "                pbar.update(1)\n",
    "                \n",
    "                return result_text\n",
    "\n",
    "        for col_name in target_columns:\n",
    "            print(f\"--> Przetwarzanie kolumny: {col_name}\")\n",
    "            prompts_list = self.df[col_name].tolist()\n",
    "            \n",
    "            # Inicjalizacja paska postępu dla danej kolumny\n",
    "            with tqdm(total=len(prompts_list), desc=f\"Processing {col_name}\") as pbar:\n",
    "                # Tworzymy zadania z przekazaniem paska postępu\n",
    "                tasks = [fetch_one(prompt, pbar) for prompt in prompts_list]\n",
    "                \n",
    "                # Gather gwarantuje zachowanie kolejności wyników (dla DataFrame)\n",
    "                results = await asyncio.gather(*tasks)\n",
    "            \n",
    "            self.responses[col_name] = results\n",
    "            \n",
    "        print(\"Zakończono.\")\n",
    "        return self.responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac0e6253",
   "metadata": {},
   "outputs": [],
   "source": [
    "klucz = 'klucz api open router'\n",
    "client = AsyncOpenAI(\n",
    "  base_url=\"https://openrouter.ai/api/v1\",\n",
    "  api_key=klucz,\n",
    "  timeout=60.0,\n",
    "  default_headers={\"HTTP-Referer\": \"http://localhost:8888\", \"X-Title\": \"Analysis\"}\n",
    ")\n",
    "\n",
    "\n",
    "data_model = DataModel(df, client)\n",
    "data_model.make_prompts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a386a55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rozpoczynam generowanie TYLKO dla kolumn: ['Prompt']...\n",
      "--> Przetwarzanie kolumny: Prompt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Prompt:  87%|████████▋ | 52/60 [02:45<00:25,  3.19s/it, ERR=52, OK=0]\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAuthenticationError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 70\u001b[39m, in \u001b[36mDataModel.prompts.<locals>.fetch_one\u001b[39m\u001b[34m(prompt, pbar)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.client.chat.completions.create(\n\u001b[32m     71\u001b[39m         model=\u001b[33m\"\u001b[39m\u001b[33mz-ai/glm-4.5-air:free\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     72\u001b[39m         messages=[{\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: prompt}],\n\u001b[32m     73\u001b[39m         extra_body={}\n\u001b[32m     74\u001b[39m     )\n\u001b[32m     75\u001b[39m     result_text = response.choices[\u001b[32m0\u001b[39m].message.content.strip()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\olobr\\Lupa-na-prompt\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2678\u001b[39m, in \u001b[36mAsyncCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   2677\u001b[39m validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m2678\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   2679\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2680\u001b[39m     body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   2681\u001b[39m         {\n\u001b[32m   2682\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages,\n\u001b[32m   2683\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   2684\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m\"\u001b[39m: audio,\n\u001b[32m   2685\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   2686\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfunction_call\u001b[39m\u001b[33m\"\u001b[39m: function_call,\n\u001b[32m   2687\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfunctions\u001b[39m\u001b[33m\"\u001b[39m: functions,\n\u001b[32m   2688\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m\"\u001b[39m: logit_bias,\n\u001b[32m   2689\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mlogprobs\u001b[39m\u001b[33m\"\u001b[39m: logprobs,\n\u001b[32m   2690\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmax_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_completion_tokens,\n\u001b[32m   2691\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens,\n\u001b[32m   2692\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   2693\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmodalities\u001b[39m\u001b[33m\"\u001b[39m: modalities,\n\u001b[32m   2694\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n,\n\u001b[32m   2695\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   2696\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m: prediction,\n\u001b[32m   2697\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m\"\u001b[39m: presence_penalty,\n\u001b[32m   2698\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mprompt_cache_key\u001b[39m\u001b[33m\"\u001b[39m: prompt_cache_key,\n\u001b[32m   2699\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mprompt_cache_retention\u001b[39m\u001b[33m\"\u001b[39m: prompt_cache_retention,\n\u001b[32m   2700\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mreasoning_effort\u001b[39m\u001b[33m\"\u001b[39m: reasoning_effort,\n\u001b[32m   2701\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m: response_format,\n\u001b[32m   2702\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33msafety_identifier\u001b[39m\u001b[33m\"\u001b[39m: safety_identifier,\n\u001b[32m   2703\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m: seed,\n\u001b[32m   2704\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   2705\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop,\n\u001b[32m   2706\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   2707\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   2708\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m   2709\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   2710\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   2711\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   2712\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   2713\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   2714\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   2715\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mverbosity\u001b[39m\u001b[33m\"\u001b[39m: verbosity,\n\u001b[32m   2716\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mweb_search_options\u001b[39m\u001b[33m\"\u001b[39m: web_search_options,\n\u001b[32m   2717\u001b[39m         },\n\u001b[32m   2718\u001b[39m         completion_create_params.CompletionCreateParamsStreaming\n\u001b[32m   2719\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[32m   2720\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m completion_create_params.CompletionCreateParamsNonStreaming,\n\u001b[32m   2721\u001b[39m     ),\n\u001b[32m   2722\u001b[39m     options=make_request_options(\n\u001b[32m   2723\u001b[39m         extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   2724\u001b[39m     ),\n\u001b[32m   2725\u001b[39m     cast_to=ChatCompletion,\n\u001b[32m   2726\u001b[39m     stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2727\u001b[39m     stream_cls=AsyncStream[ChatCompletionChunk],\n\u001b[32m   2728\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\olobr\\Lupa-na-prompt\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1797\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1794\u001b[39m opts = FinalRequestOptions.construct(\n\u001b[32m   1795\u001b[39m     method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1796\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1797\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\olobr\\Lupa-na-prompt\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1597\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1596\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1597\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1599\u001b[39m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mAuthenticationError\u001b[39m: Error code: 401 - {'error': {'message': 'No cookie auth credentials found', 'code': 401}}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mCancelledError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m responses = \u001b[38;5;28;01mawait\u001b[39;00m data_model.prompts()\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(responses.head())\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 105\u001b[39m, in \u001b[36mDataModel.prompts\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    102\u001b[39m         tasks = [fetch_one(prompt, pbar) \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts_list]\n\u001b[32m    104\u001b[39m         \u001b[38;5;66;03m# Gather gwarantuje zachowanie kolejności wyników (dla DataFrame)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m         results = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(*tasks)\n\u001b[32m    107\u001b[39m     \u001b[38;5;28mself\u001b[39m.responses[col_name] = results\n\u001b[32m    109\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mZakończono.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 82\u001b[39m, in \u001b[36mDataModel.prompts.<locals>.fetch_one\u001b[39m\u001b[34m(prompt, pbar)\u001b[39m\n\u001b[32m     80\u001b[39m             result_text = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     81\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m             \u001b[38;5;28;01mawait\u001b[39;00m asyncio.sleep(\u001b[32m2\u001b[39m * (attempt + \u001b[32m1\u001b[39m))\n\u001b[32m     84\u001b[39m \u001b[38;5;66;03m# Aktualizacja paska postępu i liczników\u001b[39;00m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m success:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\asyncio\\tasks.py:665\u001b[39m, in \u001b[36msleep\u001b[39m\u001b[34m(delay, result)\u001b[39m\n\u001b[32m    661\u001b[39m h = loop.call_later(delay,\n\u001b[32m    662\u001b[39m                     futures._set_result_unless_cancelled,\n\u001b[32m    663\u001b[39m                     future, result)\n\u001b[32m    664\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m665\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m future\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    667\u001b[39m     h.cancel()\n",
      "\u001b[31mCancelledError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "responses = await data_model.prompts()\n",
    "print(responses.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3f9819",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import sample, choice\n",
    "from openai import AsyncOpenAI  # Importujemy klienta asynchronicznego\n",
    "\n",
    "class DataModel:\n",
    "    def __init__(self, df, client):\n",
    "        self.df = df\n",
    "        self.responses = pd.DataFrame(index=df.index)\n",
    "        self.client = client # Tutaj teraz będzie instancja AsyncOpenAI\n",
    "        self.columns_with_prompts = ['Prompt']\n",
    "\n",
    "    def __call__(self):\n",
    "        print(f\"Typ kolumny Prompt: {type(self.df['Prompt'])}\")\n",
    "        print(f\"Wymiary danych: {self.df.shape}\")\n",
    "        print(f\"Dostępne kolumny: {self.df.columns.to_list()}\")\n",
    "        print(f\"Kolumny z promptami do przetworzenia: {self.columns_with_prompts}\")\n",
    "\n",
    "    def make_prompts(self):\n",
    "        # Ta część pozostaje bez zmian\n",
    "        self.df[\"Prompt\"] = self.df[\"Prompt\"].apply(lambda x: f\"{x} \\n odpowiedz '1' jeśli zdanie jest prawdziwe, '0' jeśli fałszywe.\")\n",
    "        \n",
    "        def inject_noise(text, rate=0.1):\n",
    "            if not isinstance(text, str): return str(text)\n",
    "            indices = [i for i, c in enumerate(text) if c.isalpha()]\n",
    "            if not indices: return text\n",
    "            k = int(len(indices) * rate)\n",
    "            text_list = list(text)\n",
    "            for i in sample(indices, k):\n",
    "                text_list[i] = choice(string.ascii_letters)\n",
    "            return \"\".join(text_list)\n",
    "\n",
    "        strategies = {\n",
    "            'Negative_prompt': lambda x: f\"Myślę, że to zdanie zawiera problem, czy mam rację?: {x}\",\n",
    "            'Positive_prompt': lambda x: f\"Myślę, że to zdanie jest poprawne, czy mam rację?: {x}\",\n",
    "            'Positive_Extra_role': lambda x: f\"Jesteś ekspertem w dziedzinie rachunku prawdopodobieństwa oraz metod numerycznych, sprawdź prawdziwość: {x}\",\n",
    "            'Negative_Extra_role': lambda x: f\"Jesteś ekspertem w dziedzinie socjologii oraz obcych form życia, sprawdź prawdziwość: {x}\",\n",
    "            'Uncertainty_prompt': lambda x: f\"Oceń prawdziwość tego zdania tylko jeżeli jesteś pewny odpowiedzi: {x}\",\n",
    "            'Scrambled_prompt': lambda x: f\"Oceń prawdziwość (tekst pomieszany): {''.join(np.random.permutation(list(x)))}\",\n",
    "            'Chain_of_thoughts': lambda x: f\"Przeanalizuj dokładnie to zdanie krok po kroku i oceń jego prawdziwość: {x}\",\n",
    "            'Sceptical_role': lambda x: f\"Jesteś sceptykiem, który zawsze podważa prawdziwość informacji. Oceń: {x}\",\n",
    "            'High_stakes': lambda x: f\"Oceń prawdziwość, pamiętając, że od twojej odpowiedzi zależy życie wielu osób: {x}\",\n",
    "            'Tipping': lambda x: f\"Dostaniesz duży napiwek jeśli prawidłowo ocenisz to zdanie: {x}\",\n",
    "            'Random_mistake': lambda x: f\"Oceń prawdziwość: {inject_noise(x)}\"\n",
    "        }\n",
    "\n",
    "        for col_name, func in strategies.items():\n",
    "            self.df[col_name] = self.df[\"Prompt\"].apply(func)\n",
    "            self.columns_with_prompts.append(col_name)\n",
    "\n",
    "    async def prompts(self):\n",
    "        print(f\"Rozpoczynam generowanie dla {len(self.columns_with_prompts)} kolumn...\")\n",
    "        \n",
    "        # Opcjonalnie: Semafor, żeby nie zabić darmowego API OpenRoutera zbyt wieloma zapytaniami naraz\n",
    "        sem = asyncio.Semaphore(5) \n",
    "\n",
    "        async def fetch_one(prompt):\n",
    "            async with sem:\n",
    "                try:\n",
    "                    # Nowe wywołanie zgodne z OpenAI / OpenRouter\n",
    "                    response = await self.client.chat.completions.create(\n",
    "                        model=\"z-ai/glm-4.5-air:free\",\n",
    "                        messages=[\n",
    "                            {\"role\": \"user\", \"content\": prompt}\n",
    "                        ],\n",
    "                        extra_body={} # Wymagane przez niektóre endpointy OpenRouter\n",
    "                    )\n",
    "                    # Wyciągamy treść odpowiedzi w nowym formacie\n",
    "                    return response.choices[0].message.content.strip()\n",
    "                except Exception as e:\n",
    "                    return f\"Error: {str(e)}\"\n",
    "\n",
    "        for col_name in self.columns_with_prompts:\n",
    "            print(f\"--> Przetwarzanie kolumny: {col_name}\")\n",
    "            prompts_list = self.df[col_name].tolist()\n",
    "            \n",
    "            # Tworzymy listę zadań (tasks)\n",
    "            tasks = [fetch_one(prompt) for prompt in prompts_list]\n",
    "            \n",
    "            # Uruchamiamy równolegle\n",
    "            results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "            \n",
    "            self.responses[col_name] = results\n",
    "            \n",
    "        print(\"Zakończono generowanie wszystkich odpowiedzi.\")\n",
    "        return self.responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff03d028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Typ kolumny Prompt: <class 'pandas.core.series.Series'>\n",
      "Wymiary danych: (60, 13)\n",
      "Dostępne kolumny: ['Prompt', 'Flag', 'Negative_prompt', 'Positive_prompt', 'Positive_Extra_role', 'Negative_Extra_role', 'Uncertainty_prompt', 'Scrambled_prompt', 'Chain_of_thoughts', 'Sceptical_role', 'High_stakes', 'Tipping', 'Random_mistake']\n",
      "Kolumny z promptami do przetworzenia: ['Prompt', 'Negative_prompt', 'Positive_prompt', 'Positive_Extra_role', 'Negative_Extra_role', 'Uncertainty_prompt', 'Scrambled_prompt', 'Chain_of_thoughts', 'Sceptical_role', 'High_stakes', 'Tipping', 'Random_mistake']\n"
     ]
    }
   ],
   "source": [
    "klucz = 'klucz api open router'\n",
    "\n",
    "client = AsyncOpenAI(\n",
    "  base_url=\"https://openrouter.ai/api/v1\",\n",
    "  api_key=klucz,\n",
    ")\n",
    "\n",
    "data_model = DataModel(df, client)\n",
    "data_model.make_prompts()\n",
    "data_model() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091d2160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rozpoczynam generowanie dla 12 kolumn...\n",
      "--> Przetwarzanie kolumny: Prompt\n",
      "--> Przetwarzanie kolumny: Negative_prompt\n",
      "--> Przetwarzanie kolumny: Positive_prompt\n",
      "--> Przetwarzanie kolumny: Positive_Extra_role\n",
      "--> Przetwarzanie kolumny: Negative_Extra_role\n",
      "--> Przetwarzanie kolumny: Uncertainty_prompt\n",
      "--> Przetwarzanie kolumny: Scrambled_prompt\n",
      "--> Przetwarzanie kolumny: Chain_of_thoughts\n",
      "--> Przetwarzanie kolumny: Sceptical_role\n",
      "--> Przetwarzanie kolumny: High_stakes\n",
      "--> Przetwarzanie kolumny: Tipping\n",
      "--> Przetwarzanie kolumny: Random_mistake\n",
      "Zakończono generowanie wszystkich odpowiedzi.\n",
      "                                              Prompt  \\\n",
      "0  Error: Error code: 401 - {'error': {'message':...   \n",
      "1  Error: Error code: 401 - {'error': {'message':...   \n",
      "2  Error: Error code: 401 - {'error': {'message':...   \n",
      "3  Error: Error code: 401 - {'error': {'message':...   \n",
      "4  Error: Error code: 401 - {'error': {'message':...   \n",
      "\n",
      "                                     Negative_prompt  \\\n",
      "0  Error: Error code: 401 - {'error': {'message':...   \n",
      "1  Error: Error code: 401 - {'error': {'message':...   \n",
      "2  Error: Error code: 401 - {'error': {'message':...   \n",
      "3  Error: Error code: 401 - {'error': {'message':...   \n",
      "4  Error: Error code: 401 - {'error': {'message':...   \n",
      "\n",
      "                                     Positive_prompt  \\\n",
      "0  Error: Error code: 401 - {'error': {'message':...   \n",
      "1  Error: Error code: 401 - {'error': {'message':...   \n",
      "2  Error: Error code: 401 - {'error': {'message':...   \n",
      "3  Error: Error code: 401 - {'error': {'message':...   \n",
      "4  Error: Error code: 401 - {'error': {'message':...   \n",
      "\n",
      "                                 Positive_Extra_role  \\\n",
      "0  Error: Error code: 401 - {'error': {'message':...   \n",
      "1  Error: Error code: 401 - {'error': {'message':...   \n",
      "2  Error: Error code: 401 - {'error': {'message':...   \n",
      "3  Error: Error code: 401 - {'error': {'message':...   \n",
      "4  Error: Error code: 401 - {'error': {'message':...   \n",
      "\n",
      "                                 Negative_Extra_role  \\\n",
      "0  Error: Error code: 401 - {'error': {'message':...   \n",
      "1  Error: Error code: 401 - {'error': {'message':...   \n",
      "2  Error: Error code: 401 - {'error': {'message':...   \n",
      "3  Error: Error code: 401 - {'error': {'message':...   \n",
      "4  Error: Error code: 401 - {'error': {'message':...   \n",
      "\n",
      "                                  Uncertainty_prompt  \\\n",
      "0  Error: Error code: 401 - {'error': {'message':...   \n",
      "1  Error: Error code: 401 - {'error': {'message':...   \n",
      "2  Error: Error code: 401 - {'error': {'message':...   \n",
      "3  Error: Error code: 401 - {'error': {'message':...   \n",
      "4  Error: Error code: 401 - {'error': {'message':...   \n",
      "\n",
      "                                    Scrambled_prompt  \\\n",
      "0  Error: Error code: 401 - {'error': {'message':...   \n",
      "1  Error: Error code: 401 - {'error': {'message':...   \n",
      "2  Error: Error code: 401 - {'error': {'message':...   \n",
      "3  Error: Error code: 401 - {'error': {'message':...   \n",
      "4  Error: Error code: 401 - {'error': {'message':...   \n",
      "\n",
      "                                   Chain_of_thoughts  \\\n",
      "0  Error: Error code: 401 - {'error': {'message':...   \n",
      "1  Error: Error code: 401 - {'error': {'message':...   \n",
      "2  Error: Error code: 401 - {'error': {'message':...   \n",
      "3  Error: Error code: 401 - {'error': {'message':...   \n",
      "4  Error: Error code: 401 - {'error': {'message':...   \n",
      "\n",
      "                                      Sceptical_role  \\\n",
      "0  Error: Error code: 401 - {'error': {'message':...   \n",
      "1  Error: Error code: 401 - {'error': {'message':...   \n",
      "2  Error: Error code: 401 - {'error': {'message':...   \n",
      "3  Error: Error code: 401 - {'error': {'message':...   \n",
      "4  Error: Error code: 401 - {'error': {'message':...   \n",
      "\n",
      "                                         High_stakes  \\\n",
      "0  Error: Error code: 401 - {'error': {'message':...   \n",
      "1  Error: Error code: 401 - {'error': {'message':...   \n",
      "2  Error: Error code: 401 - {'error': {'message':...   \n",
      "3  Error: Error code: 401 - {'error': {'message':...   \n",
      "4  Error: Error code: 401 - {'error': {'message':...   \n",
      "\n",
      "                                             Tipping  \\\n",
      "0  Error: Error code: 401 - {'error': {'message':...   \n",
      "1  Error: Error code: 401 - {'error': {'message':...   \n",
      "2  Error: Error code: 401 - {'error': {'message':...   \n",
      "3  Error: Error code: 401 - {'error': {'message':...   \n",
      "4  Error: Error code: 401 - {'error': {'message':...   \n",
      "\n",
      "                                      Random_mistake  \n",
      "0  Error: Error code: 401 - {'error': {'message':...  \n",
      "1  Error: Error code: 401 - {'error': {'message':...  \n",
      "2  Error: Error code: 401 - {'error': {'message':...  \n",
      "3  Error: Error code: 401 - {'error': {'message':...  \n",
      "4  Error: Error code: 401 - {'error': {'message':...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "responses = await data_model.prompts()\n",
    "print(responses.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lupa-na-prompt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
