{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09f08c61",
   "metadata": {},
   "source": [
    "## ETAP 2: Analiza wzorców i ryzyka halucynacji (Llama)\n",
    "\n",
    "Porównanie 12 strategii promptu: wpływ na accuracy, zmienność odpowiedzi, detekcja ryzyka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3431ab06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ipywidgets as widgets\n",
    "from pathlib import Path\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3141e303",
   "metadata": {},
   "source": [
    "### 1. Przygotowanie danych i metryki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7f71ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_df = pd.read_csv(\"saved_responses_llama/parsed_responses.csv\", index_col=0)\n",
    "raw_df = pd.read_csv(\"saved_responses_llama/raw_responses.csv\", index_col=0)\n",
    "df = pd.read_csv('prompts2.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2b9aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "saved_files = glob('saved_responses_llama/parsed_responses.csv')\n",
    "if not saved_files:\n",
    "    raise FileNotFoundError(\"Brak plików wyników. Uruchom najpierw llama_generation.ipynb.\")\n",
    "\n",
    "latest_file = max(saved_files, key=Path)\n",
    "parsed_df = pd.read_csv(latest_file, index_col=0)\n",
    "print(f\"[Wczytano] {Path(latest_file).name} - {parsed_df.shape}\")\n",
    "\n",
    "df = pd.read_csv('prompts2.csv', sep=';')\n",
    "y_true = df['Flag'].values\n",
    "print(f\"[Dataset] {len(y_true)} promptów ({(y_true==1).sum()} prawda, {(y_true==0).sum()} fałsz)\")\n",
    "\n",
    "metrics_list = []\n",
    "for col in parsed_df.columns:\n",
    "    y_pred = parsed_df[col].values\n",
    "    valid_mask = ~np.isnan(y_pred)\n",
    "    \n",
    "    if valid_mask.sum() > 0:\n",
    "        y_true_v = y_true[valid_mask]\n",
    "        y_pred_v = y_pred[valid_mask]\n",
    "        \n",
    "        metrics_list.append({\n",
    "            'Strategy': col,\n",
    "            'Accuracy': accuracy_score(y_true_v, y_pred_v),\n",
    "            'Precision': precision_score(y_true_v, y_pred_v, zero_division=0),\n",
    "            'Recall': recall_score(y_true_v, y_pred_v, zero_division=0),\n",
    "            'F1': f1_score(y_true_v, y_pred_v, zero_division=0),\n",
    "            'NaN': (~valid_mask).sum()\n",
    "        })\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_list).sort_values('Accuracy', ascending=False)\n",
    "print(f\"\\n[Metryki] {len(metrics_df)} strategii\")\n",
    "print(metrics_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8583ea7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[Zmienność inter-strategiowa]\")\n",
    "\n",
    "variability_data = []\n",
    "print(f\"Liczba strategii: {len(parsed_df.columns)-1}\")\n",
    "\n",
    "for idx in range(len(parsed_df)):\n",
    "    row = parsed_df.iloc[idx].dropna()\n",
    "    \n",
    "    if len(row) > 0:\n",
    "        unique_preds = row.unique()\n",
    "        has_conflict = len(unique_preds) > 1\n",
    "        agreements_with_true = (row == y_true[idx]).sum()\n",
    "        disagreements_with_true = (row != y_true[idx]).sum()\n",
    "        \n",
    "        variability_data.append({\n",
    "            'Idx': idx,\n",
    "            'Prompt': df.iloc[idx]['Prompt'],\n",
    "            'True': int(y_true[idx]),\n",
    "            'Unique_Preds': len(unique_preds),\n",
    "            'Has_Conflict': has_conflict,\n",
    "            'Std': row.std(),\n",
    "            'Consensus': 'STRONG' if disagreements_with_true == 0 or agreements_with_true == 0 else 'WEAK'\n",
    "        })\n",
    "\n",
    "var_df = pd.DataFrame(variability_data).sort_values('Std', ascending=False)\n",
    "\n",
    "print(f\"Top 10 konfliktów:\")\n",
    "print(var_df[var_df['Has_Conflict']].head(10).to_string(index=False))\n",
    "\n",
    "print(f\"\\nStatystyka: {var_df['Has_Conflict'].sum()}/{len(var_df)} konfliktów ({var_df['Has_Conflict'].sum()/len(var_df)*100:.1f}%), std={var_df['Std'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b986b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[Zmiana odpowiedzi w porównaniu do Prompt]\")\n",
    "\n",
    "baseline_col = 'Prompt'\n",
    "if baseline_col not in parsed_df.columns:\n",
    "    print(f\"Brak kolumny '{baseline_col}'\")\n",
    "else:\n",
    "    comparison_list = []\n",
    "    \n",
    "    for col in parsed_df.columns:\n",
    "        if col == baseline_col:\n",
    "            continue\n",
    "        \n",
    "        mask = (~np.isnan(parsed_df[baseline_col])) & (~np.isnan(parsed_df[col]))\n",
    "        if mask.sum() > 0:\n",
    "            changes = (parsed_df[baseline_col][mask] != parsed_df[col][mask]).sum()\n",
    "            pct = changes / mask.sum() * 100\n",
    "            agreement = (parsed_df[baseline_col][mask] == parsed_df[col][mask]).sum() / mask.sum() * 100\n",
    "            \n",
    "            comparison_list.append({\n",
    "                'Strategy': col,\n",
    "                'Zgodność_z_Prompt_%': agreement,\n",
    "                'Różnica_od_Prompt_%': pct,\n",
    "                'Próbek': mask.sum()\n",
    "            })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_list).sort_values('Różnica_od_Prompt_%', ascending=False)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    colors = plt.cm.RdYlGn(comparison_df['Zgodność_z_Prompt_%'] / 100)\n",
    "    ax.barh(comparison_df['Strategy'], comparison_df['Różnica_od_Prompt_%'], color=colors)\n",
    "    ax.set_xlabel('% zmian w stosunku do Prompt')\n",
    "    ax.set_title('Zmienność strategii - porównanie z Prompt (baseline)')\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    for i, (s, diff, agr) in enumerate(zip(comparison_df['Strategy'], \n",
    "                                            comparison_df['Różnica_od_Prompt_%'],\n",
    "                                            comparison_df['Zgodność_z_Prompt_%'])):\n",
    "        ax.text(diff + 0.5, i, f'{diff:.1f}% (∑{agr:.0f}%)', va='center', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nZestawienie zmian vs Prompt:\")\n",
    "    print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf5aba4",
   "metadata": {},
   "source": [
    "### 2. Interfejs interaktywny - eksploracja przypadków"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a33ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interactive_explorer():\n",
    "    output_area = widgets.Output()\n",
    "    \n",
    "    idx_slider = widgets.IntSlider(\n",
    "        value=0,\n",
    "        min=0,\n",
    "        max=len(parsed_df)-1,\n",
    "        description='Prompt ID:',\n",
    "        continuous_update=False\n",
    "    )\n",
    "    \n",
    "    def update_display(idx):\n",
    "        with output_area:\n",
    "            output_area.clear_output()\n",
    "            \n",
    "            original_prompt = df.iloc[idx]['Prompt']\n",
    "            ground_truth = int(y_true[idx])\n",
    "            \n",
    "            print(f\"[Prompt #{idx}] {original_prompt}\")\n",
    "            print(f\"Ground Truth: {ground_truth}\\n\")\n",
    "            \n",
    "            results = []\n",
    "            for strategy in parsed_df.columns:\n",
    "                pred = parsed_df[strategy].iloc[idx]\n",
    "                if not np.isnan(pred):\n",
    "                    pred = int(pred)\n",
    "                    is_correct = (pred == ground_truth)\n",
    "                    status = 'OK' if is_correct else 'FAIL'\n",
    "                    results.append(f\"  {strategy:25s} -> {pred} [{status}]\")\n",
    "                else:\n",
    "                    results.append(f\"  {strategy:25s} -> NaN [ERROR]\")\n",
    "            \n",
    "            print(\"\\n\".join(results))\n",
    "            \n",
    "            row_var = var_df[var_df['Idx'] == idx]\n",
    "            if len(row_var) > 0:\n",
    "                print(f\"\\nZmienność: std={row_var['Std'].values[0]:.3f}, unikalne={int(row_var['Unique_Preds'].values[0])}\")\n",
    "    \n",
    "    idx_slider.observe(lambda change: update_display(change['new']), names='value')\n",
    "    update_display(0)\n",
    "    \n",
    "    display(idx_slider, output_area)\n",
    "\n",
    "create_interactive_explorer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d10bf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "correctness_matrix = np.zeros((len(parsed_df.columns), len(parsed_df)))\n",
    "for i, strategy in enumerate(parsed_df.columns):\n",
    "    for j in range(len(parsed_df)):\n",
    "        pred = parsed_df[strategy].iloc[j]\n",
    "        if np.isnan(pred):\n",
    "            correctness_matrix[i][j] = -1\n",
    "        else:\n",
    "            correctness_matrix[i][j] = 1 if pred == y_true[j] else 0\n",
    "\n",
    "im1 = ax1.imshow(correctness_matrix, cmap='RdYlGn', aspect='auto', vmin=-1, vmax=1)\n",
    "ax1.set_xlabel('Prompt Index')\n",
    "ax1.set_ylabel('Strategy')\n",
    "ax1.set_yticklabels(parsed_df.columns, fontsize=9)\n",
    "ax1.set_title('Heatmapa poprawności')\n",
    "plt.colorbar(im1, ax=ax1, label='Correctness')\n",
    "\n",
    "strategies = metrics_df['Strategy'].values\n",
    "accuracies = metrics_df['Accuracy'].values\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(strategies)))\n",
    "\n",
    "ax2.barh(strategies, accuracies, color=colors)\n",
    "ax2.set_xlabel('Accuracy')\n",
    "ax2.set_title('Ranking strategii')\n",
    "ax2.set_xlim(0, 1)\n",
    "for i, (s, acc) in enumerate(zip(strategies, accuracies)):\n",
    "    ax2.text(acc + 0.01, i, f'{acc:.1%}', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e5d9a4",
   "metadata": {},
   "source": [
    "### 3. Analiza wzorców: wpływ zmian promptu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8079fcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[Analiza kategorii strategii]\")\n",
    "\n",
    "categories = {\n",
    "    'Positive Framing': ['Positive_prompt'],\n",
    "    'Negative Framing': ['Negative_prompt'],\n",
    "    'Role-Playing (Expert)': ['Positive_Extra_role'],\n",
    "    'Role-Playing (Dummy)': ['Negative_Extra_role'],\n",
    "    'Chain-of-Thought': ['Chain_of_thoughts'],\n",
    "    'Uncertainty': ['Uncertainty_prompt'],\n",
    "    'Scepticism': ['Sceptical_role'],\n",
    "    'Incentive': ['Tipping', 'High_stakes'],\n",
    "    'Scramble/Noise': ['Scrambled_prompt', 'Random_mistake']\n",
    "}\n",
    "\n",
    "category_performance = []\n",
    "for cat_name, cat_strategies in categories.items():\n",
    "    cat_metrics = metrics_df[metrics_df['Strategy'].isin(cat_strategies)]\n",
    "    if len(cat_metrics) > 0:\n",
    "        avg_acc = cat_metrics['Accuracy'].mean()\n",
    "        baseline_acc = metrics_df[metrics_df['Strategy'] == 'Prompt']['Accuracy'].values[0]\n",
    "        impact = avg_acc - baseline_acc\n",
    "        \n",
    "        category_performance.append({\n",
    "            'Category': cat_name,\n",
    "            'Avg_Accuracy': avg_acc,\n",
    "            'Baseline': baseline_acc,\n",
    "            'Impact': impact,\n",
    "            'N_Strategies': len(cat_strategies)\n",
    "        })\n",
    "        \n",
    "        impact_text = \"pomaga\" if impact > 0.02 else \"szkodzi\" if impact < -0.02 else \"neutralne\"\n",
    "        print(f\"{cat_name:25s} | {avg_acc:.1%} | {impact:+.1%} [{impact_text}]\")\n",
    "\n",
    "cat_perf_df = pd.DataFrame(category_performance).sort_values('Impact', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b738edce",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "colors = ['green' if x > 0 else 'red' for x in cat_perf_df['Impact']]\n",
    "ax.barh(cat_perf_df['Category'], cat_perf_df['Impact'], color=colors, alpha=0.7)\n",
    "ax.axvline(0, color='black', linestyle='-', linewidth=1)\n",
    "ax.set_xlabel('Impact vs Baseline')\n",
    "ax.set_title('Wpływ kategorii na accuracy')\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "for i, (cat, imp) in enumerate(zip(cat_perf_df['Category'], cat_perf_df['Impact'])):\n",
    "    ax.text(imp + (0.01 if imp > 0 else -0.01), i, f'{imp:+.1%}', \n",
    "            ha='left' if imp > 0 else 'right', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n[Wzorce: X -> Y]\")\n",
    "patterns = []\n",
    "for _, row in cat_perf_df.iterrows():\n",
    "    if abs(row['Impact']) > 0.02:\n",
    "        direction = \"zwiększa\" if row['Impact'] > 0 else \"zmniejsza\"\n",
    "        patterns.append(f\"Dodanie '{row['Category']}' -> {direction} accuracy o {abs(row['Impact'])*100:.1f}pp\")\n",
    "\n",
    "for p in patterns:\n",
    "    print(f\"  {p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7ba440",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[Korelacja: trudność vs zmienność]\")\n",
    "\n",
    "baseline_per_prompt = []\n",
    "for idx in range(len(parsed_df)):\n",
    "    row = parsed_df.iloc[idx].dropna()\n",
    "    if len(row) > 0:\n",
    "        correct = (row == y_true[idx]).sum()\n",
    "        pct = correct / len(row)\n",
    "        baseline_per_prompt.append(pct)\n",
    "    else:\n",
    "        baseline_per_prompt.append(0)\n",
    "\n",
    "baseline_per_prompt = np.array(baseline_per_prompt)\n",
    "prompt_difficulty = 1 - baseline_per_prompt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "ax1.scatter(prompt_difficulty, var_df.sort_values('Idx')['Std'].values, alpha=0.6, s=100)\n",
    "z = np.polyfit(prompt_difficulty, var_df.sort_values('Idx')['Std'].values, 1)\n",
    "p = np.poly1d(z)\n",
    "ax1.plot(prompt_difficulty, p(prompt_difficulty), \"r--\", alpha=0.8, linewidth=2)\n",
    "ax1.set_xlabel('Trudność')\n",
    "ax1.set_ylabel('Zmienność (std)')\n",
    "ax1.set_title('Trudne prompty -> wyższa zmienność?')\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "easy_prompts = baseline_per_prompt > 0.8\n",
    "hard_prompts = baseline_per_prompt < 0.5\n",
    "medium_prompts = ~(easy_prompts | hard_prompts)\n",
    "\n",
    "data_easy = var_df.sort_values('Idx')['Std'].iloc[easy_prompts]\n",
    "data_medium = var_df.sort_values('Idx')['Std'].iloc[medium_prompts]\n",
    "data_hard = var_df.sort_values('Idx')['Std'].iloc[hard_prompts]\n",
    "\n",
    "bp = ax2.boxplot([data_easy, data_medium, data_hard], \n",
    "                   labels=['Easy\\n(>80%)', 'Medium\\n(50-80%)', 'Hard\\n(<50%)'],\n",
    "                   patch_artist=True)\n",
    "for patch, color in zip(bp['boxes'], ['green', 'yellow', 'red']):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.5)\n",
    "\n",
    "ax2.set_ylabel('Zmienność (std)')\n",
    "ax2.set_title('Zmienność wg trudności')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Trudne prompty: {hard_prompts.sum()}, std={data_hard.mean():.3f}\")\n",
    "print(f\"Łatwe prompty: {easy_prompts.sum()}, std={data_easy.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dd3e3a",
   "metadata": {},
   "source": [
    "### 4. Detekcja halucynacji i ryzyka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331bce08",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[Detekcja halucynacji]\")\n",
    "\n",
    "hallucination_threshold = var_df['Std'].quantile(0.75)\n",
    "print(f\"Próg halucynacji (std > {hallucination_threshold:.3f}): Top 25%\")\n",
    "\n",
    "var_df_copy = var_df.copy()\n",
    "var_df_copy['Hallucination_Risk'] = var_df_copy['Std'] > hallucination_threshold\n",
    "var_df_copy['Error_Rate'] = var_df_copy.apply(\n",
    "    lambda x: 1 - sum(parsed_df.iloc[int(x['Idx'])].dropna() == y_true[int(x['Idx'])]) / len(parsed_df.iloc[int(x['Idx'])].dropna()),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "high_risk = var_df_copy[var_df_copy['Hallucination_Risk']].sort_values('Std', ascending=False)\n",
    "print(f\"\\nPrzypadki wysokiego ryzyka: {len(high_risk)}\")\n",
    "print(\"\\nTop 5:\")\n",
    "\n",
    "for i, (_, row) in enumerate(high_risk.head(5).iterrows(), 1):\n",
    "    idx = int(row['Idx'])\n",
    "    prompt = df.iloc[idx]['Prompt']\n",
    "    print(f\"{i}. #{idx}: '{prompt}\")\n",
    "    print(f\"   Std={row['Std']:.3f}, True={int(row['True'])}\")\n",
    "    \n",
    "    row_preds = parsed_df.iloc[idx].dropna()\n",
    "    if len(row_preds) > 0:\n",
    "        pred_counts = row_preds.value_counts().to_dict()\n",
    "        pred_str = \", \".join([f\"{int(k)}:{v}x\" for k, v in sorted(pred_counts.items())])\n",
    "        print(f\"   Odpowiedzi: {pred_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e5d4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[Ryzyko per strategia]\")\n",
    "\n",
    "risk_matrix = []\n",
    "for strategy in parsed_df.columns:\n",
    "    strategy_volatility = []\n",
    "    \n",
    "    for idx in range(len(parsed_df)):\n",
    "        inter_var = var_df[var_df['Idx'] == idx]['Std'].values\n",
    "        if len(inter_var) > 0 and inter_var[0] > hallucination_threshold:\n",
    "            row_preds = parsed_df.iloc[idx].dropna()\n",
    "            if len(row_preds) > 1:\n",
    "                pred = parsed_df[strategy].iloc[idx]\n",
    "                if not np.isnan(pred):\n",
    "                    majority = row_preds.mode().values\n",
    "                    if len(majority) > 0:\n",
    "                        agrees = pred == majority[0]\n",
    "                        strategy_volatility.append(not agrees)\n",
    "    \n",
    "    if len(strategy_volatility) > 0:\n",
    "        risk_score = sum(strategy_volatility) / len(strategy_volatility)\n",
    "    else:\n",
    "        risk_score = 0\n",
    "    \n",
    "    risk_matrix.append({\n",
    "        'Strategy': strategy,\n",
    "        'Hallucination_Risk': risk_score,\n",
    "        'Base_Accuracy': metrics_df[metrics_df['Strategy'] == strategy]['Accuracy'].values[0]\n",
    "    })\n",
    "\n",
    "risk_df = pd.DataFrame(risk_matrix).sort_values('Hallucination_Risk', ascending=False)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "colors_risk = plt.cm.RdYlGn_r(risk_df['Hallucination_Risk'] / risk_df['Hallucination_Risk'].max())\n",
    "ax1.barh(risk_df['Strategy'], risk_df['Hallucination_Risk'], color=colors_risk)\n",
    "ax1.set_xlabel('Risk Score')\n",
    "ax1.set_title('Ryzyko halucynacji per strategia')\n",
    "ax1.set_xlim(0, 1)\n",
    "\n",
    "for i, (s, risk) in enumerate(zip(risk_df['Strategy'], risk_df['Hallucination_Risk'])):\n",
    "    ax1.text(risk + 0.02, i, f'{risk:.1%}', va='center', fontsize=9)\n",
    "\n",
    "ax2.scatter(risk_df['Hallucination_Risk'], risk_df['Base_Accuracy'], s=200, alpha=0.6)\n",
    "for idx, row in risk_df.iterrows():\n",
    "    ax2.annotate(row['Strategy'], \n",
    "                (row['Hallucination_Risk'], row['Base_Accuracy']),\n",
    "                fontsize=8, alpha=0.7)\n",
    "\n",
    "ax2.set_xlabel('Hallucination Risk')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Trade-off: Ryzyko vs Dokładność')\n",
    "ax2.grid(alpha=0.3)\n",
    "ax2.axhline(risk_df['Base_Accuracy'].mean(), color='gray', linestyle='--', alpha=0.5)\n",
    "ax2.axvline(risk_df['Hallucination_Risk'].mean(), color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 5 strategii wysokiego ryzyka:\")\n",
    "print(risk_df.head(5)[['Strategy', 'Hallucination_Risk', 'Base_Accuracy']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc269b3",
   "metadata": {},
   "source": [
    "### 5. Raport końcowy i export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8a3b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "print(\"\\n[Raport końcowy]\")\n",
    "print(f\"Data: {datetime.now().strftime('%Y-%m-%d %H:%M')}\")\n",
    "print(f\"Model: Llama 3.2 3B\")\n",
    "print(f\"Dataset: {len(df)} promptów, {len(parsed_df.columns)} strategii\")\n",
    "\n",
    "baseline_acc = metrics_df[metrics_df['Strategy'] == 'Prompt']['Accuracy'].values[0]\n",
    "best_strategy = metrics_df.iloc[0]\n",
    "worst_strategy = metrics_df.iloc[-1]\n",
    "\n",
    "print(f\"\\n1. Ranking strategii\")\n",
    "print(metrics_df[['Strategy', 'Accuracy', 'F1']].head(5).to_string(index=False))\n",
    "print(f\"\\nNajlepsza: {best_strategy['Strategy']} ({best_strategy['Accuracy']:.1%}, +{(best_strategy['Accuracy']-baseline_acc)*100:.1f}pp)\")\n",
    "print(f\"Najsłabsza: {worst_strategy['Strategy']} ({worst_strategy['Accuracy']:.1%}, {(worst_strategy['Accuracy']-baseline_acc)*100:.1f}pp)\")\n",
    "\n",
    "print(f\"\\n2. Wpływ kategorii\")\n",
    "print(cat_perf_df[['Category', 'Impact']].head(3).to_string(index=False))\n",
    "\n",
    "print(f\"\\n3. Halucynacje\")\n",
    "print(f\"Przypadki wysokiego ryzyka: {len(high_risk)}/{len(var_df)} ({len(high_risk)/len(var_df)*100:.1f}%)\")\n",
    "print(f\"Średnia zmienność: {var_df['Std'].mean():.3f}\")\n",
    "print(f\"Konflikty między strategiami: {var_df['Has_Conflict'].sum()}/{len(var_df)} ({var_df['Has_Conflict'].sum()/len(var_df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91492b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[Export wyników]\")\n",
    "folder_zapisu = 'results_llama'\n",
    "\n",
    "import os\n",
    "os.makedirs(folder_zapisu, exist_ok=True)\n",
    "\n",
    "metrics_df.to_csv(f'{folder_zapisu}/ranking_strategies.csv', index=False)\n",
    "print(f\"Ranking: ranking_strategies.csv\")\n",
    "\n",
    "high_risk.to_csv(f'{folder_zapisu}/hallucination_cases.csv', index=False)\n",
    "print(f\"Halucynacje: hallucination_cases.csv\")\n",
    "risk_df.to_csv(f'{folder_zapisu}/risk_indicators.csv', index=False)\n",
    "print(f\"Ryzyko: risk_indicators.csv\")\n",
    "\n",
    "var_df.to_csv(f'{folder_zapisu}/variability_analysis.csv', index=False)\n",
    "print(f\"Zmienność: variability_analysis.csv\")\n",
    "report = {\n",
    "    'metadata': {\n",
    "        'model': 'Llama 3.2 3B',\n",
    "        'dataset_size': len(df),\n",
    "        'n_strategies': len(parsed_df.columns)\n",
    "    },\n",
    "    'summary': {\n",
    "        'best_strategy': str(best_strategy['Strategy']),\n",
    "        'best_accuracy': float(best_strategy['Accuracy']),\n",
    "        'worst_strategy': str(worst_strategy['Strategy']),\n",
    "        'worst_accuracy': float(worst_strategy['Accuracy']),\n",
    "        'baseline_accuracy': float(baseline_acc),\n",
    "        'cases_with_conflict': int(var_df['Has_Conflict'].sum()),\n",
    "        'conflict_percentage': float(var_df['Has_Conflict'].sum() / len(var_df) * 100)\n",
    "    },\n",
    "    'patterns': patterns,\n",
    "    'recommendations': {\n",
    "        'best_practices': ['Chain-of-Thought', 'Expert Role', 'Positive Framing'],\n",
    "        'avoid': ['Scramble', 'Negative Role', 'Uncertainty without CoT']\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f'{folder_zapisu}/report.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(report, f, ensure_ascii=False, indent=2)\n",
    "print(f\"Report: report.json\")\n",
    "\n",
    "print(f\"\\n[ZAKONCZONO] Wszystkie wyniki w {folder_zapisu}/\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
